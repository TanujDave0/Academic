{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models with EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fill in the code to implement log_sum_exp and loglikelihood function, that outputs the log likelihood of given data X.\n",
    "2. Fill in the code to implement EM algorithm, following the instructions to fill in the code of E-step and M-step.\n",
    "3. We will now try different number of iterations. Fill in the code for running EM algorithm and plotting. \n",
    "4. Report how many iterations that EM algorithm to completion(convergence)? And observe how does loglikelihood change over iterations by plotting the loglikelihood.\n",
    "\n",
    "Hint: you can use library(from sklearn.mixture import GaussianMixture) to confirm your algorithm implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first import some modules for this problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import copy\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create synthetic data\n",
    "We have gone through all the theoretical concepts of the guassian mixture models. Here, we will generate synthetic data from a mixture of three Gaussians, and then we visualize generated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_data, means, covariances, weights):\n",
    "    \"\"\" Creates a list of data points \"\"\"\n",
    "    num_clusters = len(weights)\n",
    "    data = []\n",
    "    for i in range(num_data):\n",
    "        #  Use np.random.choice and weights to pick a cluster id greater than or equal to 0 and less than num_clusters.\n",
    "        k = np.random.choice(len(weights), 1, p=weights)[0]\n",
    "\n",
    "        # Use np.random.multivariate_normal to create data from this cluster\n",
    "        x = np.random.multivariate_normal(means[k], covariances[k])\n",
    "\n",
    "        data.append(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for creating synthetic data\n",
    "init_means = [\n",
    "    [5, 0], # mean of cluster 1\n",
    "    [1, 1], # mean of cluster 2\n",
    "    [0, 5]  # mean of cluster 3\n",
    "]\n",
    "init_covariances = [\n",
    "    [[.5, 0.], [0, .5]], # covariance of cluster 1\n",
    "    [[.92, .38], [.38, .91]], # covariance of cluster 2\n",
    "    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n",
    "]\n",
    "init_weights = [1/4., 1/2., 1/4.]  # weights of each cluster\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(4)\n",
    "X = generate_data(100, init_means, init_covariances, init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plt.figure()\n",
    "d = np.vstack(X)\n",
    "plt.plot(d[:,0], d[:,1],'ko')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. E-M algorithm\n",
    "To solve the problem of which datapoints came from which guassian, we implement EM algorithm here. The way it works is that it will start by placing guassians randomly, which means that generates random mean and variance for the guassian. Then it will iterate over these two steps until it converges.\n",
    "\n",
    "### 2.1 E step: \n",
    "With the current means and variances, we calculate the probability of each data point $x_i$ coming from each guassian.\n",
    "\n",
    "### 2.2 M step:\n",
    "After getting the probability, we re-estimate the guassians' new mean and variance.\n",
    "\n",
    "Hint: you can use multivariate_normal.pdf to compute the the likelihood of seeing a data point in a multivariate Gaussian distribution and np.outer to estimate the covariance matrix from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM(data, init_means, init_covariances, init_weights, n_iter=100, thresh=1e-4):\n",
    "    \"\"\" EM algorithm implementation. \n",
    "    Input:\n",
    "    -data: a array of datapoints\n",
    "    -init_weights: a array of weights for k clusters\n",
    "    -init_means: a array of means for k clusters\n",
    "    -init_covariances:a array of covariances for k clusters\n",
    "    -n_iter: number of iterations\n",
    "    -thresh: threshold value to stop performing the EM optimization\n",
    "    Output:\n",
    "    -out: a dictonary with five elements:\n",
    "        1. 'loglik': a record of the log likelihood at each iteration\n",
    "        2. 'resp': the final responsibility matrix\n",
    "        3. 'means': a list of K means\n",
    "        4. 'covs': a list of K covariance matrices\n",
    "        5. 'weights': the weights corresponding to each model component\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make copies of initial parameters, which we will update during each iteration\n",
    "    means = init_means[:]\n",
    "    covariances = init_covariances[:]\n",
    "    weights = init_weights[:]\n",
    "    \n",
    "    # Infer dimensions of dataset and the number of clusters\n",
    "    num_data = len(data)\n",
    "    num_dim = len(data[0])\n",
    "    num_clusters = len(means)\n",
    "    \n",
    "    # Initialize some useful variables\n",
    "    resp = np.zeros((num_data, num_clusters))\n",
    "    loglike = loglikelihood(data, weights, means, covariances)\n",
    "    loglike_trace = [loglike]\n",
    "    loglike_new = 0\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        # E-step: compute responsibilities\n",
    "        # Update resp matrix so that resp[j, k] is the responsibility of cluster k for data point j.\n",
    "        # Hint: To compute likelihood of seeing data point j given cluster k, use multivariate_normal.pdf.\n",
    "        for j in range(num_data):\n",
    "            for k in range(num_clusters):\n",
    "#                 print('Insert code here')\n",
    "                \n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "                resp[j,k] = multivariate_normal.pdf(data[j], means[k], covariances[k])\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        # M-step\n",
    "        # Compute the total responsibility assigned to each cluster, which will be useful when \n",
    "        # implementing M-steps below. \n",
    "        \n",
    "        counts = np.sum(resp, axis=0)\n",
    "        \n",
    "        for k in range(num_clusters):\n",
    "#             dd=0\n",
    "        \n",
    "            # Update the weight for cluster k using the M-step update rule for the cluster weight, \\hat{\\pi}_k.\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "#             print('Insert code here')\n",
    "            \n",
    "            \n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            \n",
    "            # Update covariances for cluster k using the M-step update rule for covariance variables.\n",
    "            # This will assign the variable covariances[k] to be the estimate for \\hat{\\Sigma}_k.\n",
    "            # Hint: Use np.outer on the data[j] and this cluster's mean\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            covariances[k] = np.outer(data,means[k])\n",
    "\n",
    "            \n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****       \n",
    "        \n",
    "        # Check for convergence in log-likelihood and store\n",
    "        if (loglike_new - loglike) < thresh and loglike_new > -np.inf:\n",
    "            break\n",
    "        loglike = loglike_new\n",
    "    \n",
    "    out = {'weights': weights, 'means': means, 'covs': covariances, 'loglik': loglike_trace, 'resp': resp}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to calculate Log Likelihood:\n",
    "\n",
    "The EM algorithm attempts to find maximum likelihood estimates for models with latent variables.\n",
    "\n",
    "Let Y be the entire set of observed variables and $Z$ the entire set of latent variables. The log-likelihood is therefore:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log (P(Y|\\Theta)) \\ = \\ \\log(\\sum_Z P(Y,Z|\\Theta))\n",
    "\\end{equation}\n",
    "where we’ve simply marginalized $Z$ out of the joint distribution.\n",
    "\n",
    "To compute the mixture of Gaussians, we introduce a set of cluster weights,  $\\pi_k$ , one for each cluster $k$ . Where  $\\sum_{i=1} ^K \\pi _k = 1$  and  $\\pi_k≥0 ∀k$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(pik):\n",
    "    \"\"\" Compute log(\\sum_i exp(pi_k)) for input array.\n",
    "    Input:\n",
    "    -pik:input arrary    \n",
    "    \"\"\"\n",
    "    return np.max(pik) + np.log(np.sum(np.exp(pik - np.max(pik))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(X, weights, means, covs):\n",
    "    \"\"\" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. \n",
    "    Input:\n",
    "    -data: a array of datapoints\n",
    "    -weights: a array of weights for k clusters\n",
    "    -means: a array of means for k clusters\n",
    "    -covs:a array of covariances for k clusters\n",
    "    \"\"\"\n",
    "    num_clusters = len(means)\n",
    "    num_dim = len(X[0])\n",
    "    \n",
    "    ll = 0\n",
    "    for d in X:\n",
    "        \n",
    "        Z = np.zeros(num_clusters) #to store new pik\n",
    "        for k in range(num_clusters):\n",
    "            \n",
    "            # Compute (x-mu)^T * Sigma^{-1} * (x-mu)\n",
    "            delta = np.array(d) - means[k]\n",
    "            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))\n",
    "            \n",
    "            # Compute loglikelihood contribution for this data point and this cluster\n",
    "            Z[k] += np.log(weights[k])\n",
    "            Z[k] -= 1/2. * (num_dim * np.log(2*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)\n",
    "            \n",
    "        # Increment loglikelihood contribution of this data point across all clusters\n",
    "        ll += log_sum_exp(Z)\n",
    "        \n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fd682b5b1ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Initialization of parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mchosen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minitial_means\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchosen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "# Initialization of parameters\n",
    "chosen = np.random.choice(len(X), 3, replace=False)\n",
    "initial_means = [X[x] for x in chosen]\n",
    "initial_covs = [np.cov(X, rowvar=0)] * 3\n",
    "initial_weights = [1/3.] * 3\n",
    "\n",
    "# Run EM \n",
    "results = EM(X, initial_means, initial_covs, initial_weights)\n",
    "\n",
    "#sample outputs:\n",
    "# weights: [0.3007102300609823, 0.17993710074247007, 0.5193526691965472]\n",
    "# means: [array([0.02138285, 4.947729  ]),array([4.94239235, 0.31365311]),array([1.08181125, 0.73903508])]\n",
    "# covs: [array([[0.2932614 , 0.05048455],[0.05048455, 0.35281537]]),array([[ 0.3556437 , -0.01494875],\n",
    "#       [-0.01494875,  0.66695025]]),array([[0.67114992, 0.33058965],[0.33058965, 0.90429724]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the results of EM implementations\n",
    "We provide a plot_contours function to visualize the gaussian componets over the data. Try to plot Gaussian componets after differenent iterations(n_iter = 5,10), and also after converged iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0,\n",
    "                 mux=0.0, muy=0.0, sigmaxy=0.0):\n",
    "    \"\"\"\n",
    "    Bivariate Gaussian distribution for equal shape *X*, *Y*.\n",
    "    \n",
    "    \"\"\"\n",
    "    Xmu = X-mux\n",
    "    Ymu = Y-muy\n",
    "\n",
    "    rho = sigmaxy/(sigmax*sigmay)\n",
    "    z = Xmu**2/sigmax**2 + Ymu**2/sigmay**2 - 2*rho*Xmu*Ymu/(sigmax*sigmay)\n",
    "    denom = 2*np.pi*sigmax*sigmay*np.sqrt(1-rho**2)\n",
    "    return np.exp(-z/(2*(1-rho**2))) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(data, means, covs, title):\n",
    "    plt.figure()\n",
    "    plt.plot([x[0] for x in data], [y[1] for y in data],'ko') # data\n",
    "\n",
    "    delta = 0.025\n",
    "    k = len(means)\n",
    "    x = np.arange(-2.0, 7.0, delta)\n",
    "    y = np.arange(-2.0, 7.0, delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    col = ['green', 'red', 'indigo']\n",
    "    for i in range(k):\n",
    "        mean = means[i]\n",
    "        cov = covs[i]\n",
    "        sigmax = np.sqrt(cov[0][0])\n",
    "        sigmay = np.sqrt(cov[1][1])\n",
    "        sigmaxy = cov[0][1]/(sigmax*sigmay)\n",
    "        bN = bivariate_normal(X, Y, sigmax, sigmay, mean[0], mean[1], sigmaxy)\n",
    "        plt.contour(X, Y, bN, colors = col[i])\n",
    "        plt.title(title)\n",
    "    plt.rcParams.update({'font.size':16})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "# running EM\n",
    "\n",
    "# plotting\n",
    "\n",
    "# For graduate students only -- Plot log-likelihood as a function of iteration number\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0dcbf37802d0b81ba6b67e8653503299cec0d48c516ca608fc56694ffda75cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
