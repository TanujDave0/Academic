{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)**\n",
    "\n",
    "**a)** If we want to maximize our return only knowing the expected return of investments E[X_0] and E[X_1], it is only logical to invest in the stock with higher expected return of investment. As investing even a fraction of capital in the stock will lower return will yield a lower return for that fraction as it would have on the other stock. Thus only knowing E[X_0] and E[X_1], α should be either 0 or 1 saying we invested all the captial in stock_0 (α = 1) or stock_1 (α = 0 as 1-α = 1). If stock_0 has higher return, α* = 1 (we invest all in stock_0) thus α* = 1-0 and similarly for α*=0 or 1-α*=1 since stock_1 had a higher return thus α*=0 (1-1).\n",
    "\n",
    "Thus optimal fraction α* will be 1 - index i of stock with higher return (index i = {0 or 1}), for this we would need the max of expected return of both the stocks or argmax E[X] and since we need the index of the stock, argmax_i={0,1} E[X_i]\n",
    "\n",
    "converting this to matematical formula gives α* = 1 - argmax_i={0,1} E[X_i]. Thus the statement is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** We know that\n",
    "    Var[a*X] = (a^2)*Var[X] for a constant 'a'\n",
    "    \n",
    "also that\n",
    "    Var[X*Y] = Var[X] + Var[Y] + 2COV[X,Y]\n",
    "\n",
    "and finally that\n",
    "    COV[aX,bY] = abCOV[X,Y]\n",
    "\n",
    "Using these two,\n",
    "\n",
    "(X = X_0  and Y = X_1)\n",
    "\n",
    "Var[aX + bY] = Var[aX] + Var[bY] + 2COV[aX, bY] = (a^2)Var[X] + (b^2)Var[Y] + 2abCOV[X, Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "(let X = X_0 and Y = X_1)\n",
    "\n",
    "We want to minimize Var[αX + (1-α)Y]\n",
    "\n",
    "Var[αX + (1-α)Y] = (α^2)Var[x] + (1-α)^2Var[Y] + 2(α-α^2)COV[X,Y]\n",
    "\n",
    "let Var[x] = a, Var[Y] = b and COV[X,Y] = c. Thus,\n",
    "\n",
    "Var[αX + (1-α)Y] = a(α^2) + b(1-α)^2 + 2(α-α^2)c = aα^2 + bα^2 - 2cα^2 - 2bα + 2cα + b\n",
    "Var[αX + (1-α)Y] = α^2(a+b-2c) - 2α(b-c) + b\n",
    "\n",
    "Since we want to minimize Var[αX + (1-α)Y], we have to minimize the quadratic α^2(a+b-2c) - 2α(b-c) + b.\n",
    "\n",
    "The minima is given at a point say x_0 for function f(x) when f'(x) = 0 (slope is 0)\n",
    "\n",
    "Diffrentiating the quadratic α^2(a+b-2c) - 2α(b-c) + b and equaling to 0 gives\n",
    "\n",
    "2α(a+b-2c)-2(b-c) = 0\n",
    "\n",
    "2α(a+b-2c) = 2(b-c)\n",
    "\n",
    "α = (b-c)/(a+b-2c)\n",
    "\n",
    "thus the minimum is at the point α = (b-c)/(a+b-2c)\n",
    "\n",
    "putting back the vaiables a,b, and c gives.\n",
    "\n",
    "α = (Var[X_1] - COV[X_0, X_1])/(Var[X_0] + Var[X_1]  -2COV[X_0, X_1]). And since this α is the minimum of Var[αX + (1-α)Y]\n",
    "\n",
    "thus α* = (Var[X_1] - COV[X_0, X_1]) / (Var[X_0] + Var[X_1]  - 2COV[X_0, X_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2)**\n",
    "a), b) in the notes on BlackBoard\n",
    "\n",
    "c) P(x_3|D) = P(D|x_3)*P(x_3)/P(D)\n",
    "P(lambda|D) for alpha = 2 beta = 1\n",
    "is 1* lambda * (e^(-lambda)) / Gamma(alpha) = lambda*(e^(lambda)) / Gamma(alpha)\n",
    "\n",
    "d) 0.2439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3)**\n",
    "a) Partitioning code at the end of the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1, 'me': 1, 'now': 1, 'tomorrow': 1})\n",
      "The original dataset contains 5572 examples in total.\n",
      "The training set contains 4457 examples.\n",
      "The testing set contains 1115 examples.\n"
     ]
    }
   ],
   "source": [
    "# set up code for this experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Download the dataset to the server\n",
    "# Import the data using the read_csv() method from pandas\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.dropbox.com/s/ybnfa73k41loqbx/SMSSpamCollection.dat.txt?dl=1'\n",
    "file_name = 'SMSSpamCollection.dat'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "df = pd.read_csv(file_name,\n",
    "                    sep='\\t',\n",
    "                    header=None,\n",
    "                    names=['label', 'sms_message'])\n",
    "df.head()\n",
    "\n",
    "# Run the next line only once after running the previous code block\n",
    "# Running it more than once will turn the labels into NaN\n",
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()\n",
    "\n",
    "def count_frequency(documents):\n",
    "    \"\"\"\n",
    "    count occurrence of each word in the document set.\n",
    "    Inputs:\n",
    "    - documents: list, each entity is a string type SMS message\n",
    "    Outputs:\n",
    "    - frequency: a dictionary. The key is the unique words, and the value is the number of occurrences of the word\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Here is the pseudo-code\n",
    "    # Step 1: covert all strings into their lower case form\n",
    "    lower_case_doc = []\n",
    "    for s in documents:\n",
    "        lower_case_doc.append(???)\n",
    "    \n",
    "    # Step 2: remove all punctuations\n",
    "    no_punc_doc = []\n",
    "    for s in lower_case_doc:\n",
    "        no_punc_doc.append(???)\n",
    "    \n",
    "    # Step 3: tokenize a sentence, i.e., split a sentence into individual words \n",
    "    # using a delimiter. The delimiter specifies what character we will use to identify the beginning \n",
    "    # and the end of a word.\n",
    "    words_doc = []\n",
    "    for s in no_punc_doc:\n",
    "        words_doc.append(???)\n",
    "    \n",
    "    # Step 4: count frequencies. To count the occurrence of each word in the document set. \n",
    "    # We can use the `Counter` method from the Python `collections` library for this purpose. \n",
    "    # `Counter` counts the occurrence of each item in the list and returns a dictionary with \n",
    "    # the key as the item being counted and the corresponding value being the count of that item in the list. \n",
    "    all_words = []\n",
    "    for s in words_doc:\n",
    "        all_words.extend(???)\n",
    "    frequency = \"some function/constructor on all_words\"\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # Step 1: covert all strings into their lower case form\n",
    "    lower_case_doc = []\n",
    "    for s in documents:\n",
    "        lower_case_doc.append(s.lower())\n",
    "    \n",
    "    # Step 2: remove all punctuations\n",
    "    no_punc_doc = []\n",
    "    for s in lower_case_doc:\n",
    "        no_punc_doc.append(s.translate(str.maketrans('','', string.punctuation)))\n",
    "    \n",
    "    # Step 3: tokenize a sentence, i.e., split a sentence into individual words \n",
    "    # using a delimiter. The delimiter specifies what character we will use to identify the beginning \n",
    "    # and the end of a word.\n",
    "    words_doc = []\n",
    "    for s in no_punc_doc:\n",
    "        words_doc.append(s.split())\n",
    "    \n",
    "    # Step 4: count frequencies. To count the occurrence of each word in the document set. \n",
    "    # We can use the `Counter` method from the Python `collections` library for this purpose. \n",
    "    # `Counter` counts the occurrence of each item in the list and returns a dictionary with \n",
    "    # the key as the item being counted and the corresponding value being the count of that item in the list. \n",
    "    all_words = []\n",
    "    for s in words_doc:\n",
    "        all_words.extend(s)\n",
    "    frequency = Counter(all_words)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return frequency\n",
    "\n",
    "# Unit test case:\n",
    "# documents = ['Hello, how are you!', \n",
    "#              'Win money, win from home.',\n",
    "#              'Call me now.',\n",
    "#              'Hello, Call hello you tomorrow?']\n",
    "# sample outputs:\n",
    "# Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1,\n",
    "# 'me': 1, 'now': 1, 'tomorrow': 1})\n",
    "documents = ['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?']\n",
    "\n",
    "freq = count_frequency(documents)\n",
    "print(freq)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# learn to read API documentation\n",
    "# you can get detailed instructions about this method through this link:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], test_size = 0.2, random_state = 1)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f'The original dataset contains {df.shape[0]} examples in total.')\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {X_test.shape[0]} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.5}\n",
      "{0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, 'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, 'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05, 'unseen_words': 4.99775101204458e-05}, 1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, 'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, 'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, 'now': 9.996001599360256e-05, 'unseen_words': 4.998000799680128e-05}}\n"
     ]
    }
   ],
   "source": [
    "def train_NB_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    training a naive bayes model from the training data.\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Output:\n",
    "    - prior: a dictionary, whose key is the class label, and value is the prior probability.\n",
    "    - conditional: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    \"\"\"\n",
    "\n",
    "    # To make your code more readable, you can implement some auxiliary functions\n",
    "    # such as `prior_prob` and `conditional_prob` outside of this train_NB_model function\n",
    "\n",
    "    # compute the prior probability\n",
    "    prior = prior_prob(y_train)\n",
    "    \n",
    "    # compute the conditional probability\n",
    "    conditional = conditional_prob(X_train, y_train)\n",
    "\n",
    "    return prior, conditional\n",
    "\n",
    "# Start your auxiliary functions\n",
    "    \n",
    "def prior_prob(y_train):\n",
    "    \"\"\"\n",
    "    compute the prior probability\n",
    "    Inputs:\n",
    "    - y_train: an array that stores ground true label for training data\n",
    "    Outputs:\n",
    "    - prior: a dictionary. key is the class label, value is the prior probability.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    n = len(y_train)\n",
    "    \n",
    "    all_words = []\n",
    "    for s in y_train:\n",
    "        all_words.append(s)\n",
    "    p = Counter(all_words)\n",
    "    \n",
    "    prior = {}\n",
    "    \n",
    "    for e in p:\n",
    "        prior[e] = p[e]/n\n",
    "    \n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return prior\n",
    "\n",
    "def conditional_prob(X_train, y_train):\n",
    "    \"\"\"\n",
    "    compute the conditional probability for a document set\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Ouputs:\n",
    "    - cond_prob: a dictionary. key is the class label, value is a dictionary in which the key is word, the value is the conditional probability of feature x_i given y.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    d = {}\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i] not in d:\n",
    "            d[y_train[i]] = []\n",
    "        \n",
    "        d[y_train[i]].append(X_train[i])\n",
    "    \n",
    "    cond_prob = {}\n",
    "    \n",
    "    for e in d:\n",
    "        counter = count_frequency(d[e])\n",
    "        n = 0\n",
    "        for c in counter:\n",
    "            n += counter[c]\n",
    "        f = {}\n",
    "        for c in counter:\n",
    "            f[c] = (counter[c]+1.0)/(n+20000)\n",
    "        f['unseen_words'] = 1.0/(n+20000)\n",
    "        \n",
    "        cond_prob[e] = f\n",
    "    \n",
    "    \n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return cond_prob\n",
    "\n",
    "# unit test case:\n",
    "# x_train = ['Hello, how are you!',\n",
    "#           'Win money, win from home.',\n",
    "#            'Call me now.',\n",
    "#            'Hello, Call hello you tomorrow?']\n",
    "# y_train = np.array([0,1,1,0])\n",
    "#\n",
    "# sample outputs:\n",
    "# prior: {0: 0.5, 1: 0.5}\n",
    "# conditional: {0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, \n",
    "#                   'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, \n",
    "#                   'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05}, \n",
    "#               1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, \n",
    "#                   'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, \n",
    "#                   'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, \n",
    "#                   'now': 9.996001599360256e-05}}\n",
    "x_train = np.array(['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?'])\n",
    "y_train_mini = np.array([0,1,1,0])\n",
    "\n",
    "prior = prior_prob(y_train_mini)\n",
    "cond_prob = conditional_prob(x_train, y_train_mini)\n",
    "print(prior)\n",
    "print(cond_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "The predict label gives an output that is very close to the expected output. The ROC Curve changes and becomes worse as the train size decreases and becomes better as the train size increases till a certain value. The output is sesative to the delimiter change in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [[0.9230591772627317, 0.07694082273726824]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "def predict_label(X_test, prior_prob, cond_prob):\n",
    "    \"\"\"\n",
    "    predict the class labels for the testing set\n",
    "    Inputs:\n",
    "    - X_test: an array of shape (num_test,) which stores test data. \n",
    "              Each entity is a string type SMS message.\n",
    "    - prior_prob: a dictionary which stores the prior probability for all categories\n",
    "              We previously used \"prior_prob\" as the name of function.  \n",
    "              Here it is used as a dictionary name.  No confusion should arise.\n",
    "    - cond_prob: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    Outputs:\n",
    "    - predict: an array that stores predicted labels\n",
    "    - test_prob: an array of shape (num_test, num_classes) which stores the posterior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    predict = []\n",
    "    test_prob = []\n",
    "    \n",
    "    for sms in X_test:\n",
    "        p = []\n",
    "        t = 0\n",
    "        \n",
    "        for label in prior_prob:\n",
    "            g = np.log(prior_prob[label])\n",
    "            am = prior_prob[label]\n",
    "            \n",
    "            s = count_frequency([sms])\n",
    "            \n",
    "            for x in s:\n",
    "                if x in cond_prob[label]:\n",
    "                    g+=np.log(cond_prob[label][x])\n",
    "                    am*=cond_prob[label][x]\n",
    "                else:\n",
    "                    g+=np.log(cond_prob[label]['unseen_words'])\n",
    "                    am*=cond_prob[label]['unseen_words']\n",
    "            t+=am\n",
    "\n",
    "            p.append(am)\n",
    "        for i in range(len(p)):\n",
    "            p[i] /= t\n",
    "        test_prob.append(p)\n",
    "        \n",
    "        predict.append(p.index(max(p)))\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return predict, test_prob\n",
    "\n",
    "\n",
    "# unit test case:\n",
    "# x_test = np.array(['Hello, how are you!'])\n",
    "# sample outputs:\n",
    "# y_pred: [0] \n",
    "# prob: [[0.92298104 0.07701896]]\n",
    "x_test = np.array(['Hi, how are you today!'])\n",
    "y_pred, test_prob = predict_label(x_test, prior, cond_prob)\n",
    "print(y_pred, test_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57190293472761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import sklearn.metrics\n",
    "\n",
    "def compute_metrics(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    compute the performance metrics\n",
    "    Inputs:\n",
    "    - y_pred: an array of predictions\n",
    "    - y_true: an array of ground true labels\n",
    "    Outputs:\n",
    "    - acc: accuracy\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return acc\n",
    "\n",
    "# unit test case:\n",
    "# y_pred = np.array([0,1,1,1,0,1])\n",
    "# y_true = np.array([0,1,0,0,1,1])\n",
    "# \n",
    "# sample outputs:\n",
    "# acc: 0.5 \n",
    "# cm: [[1 2]\n",
    "#      [1 2]] \n",
    "# f1: 0.5714285714285715\n",
    "y_pred = np.array([0,1,1,1,0,1])\n",
    "y_true = np.array([0,1,0,0,1,1])\n",
    "acc = compute_metrics(y_pred, y_true)\n",
    "print(acc)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "  plt.plot(fpr, tpr, color='blue', label='ROC')\n",
    "  plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "  plt.xlabel('FPR')\n",
    "  plt.ylabel('TPR')\n",
    "  plt.title('ROC Curve')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# We now compute the test performance.\n",
    "# X_train, X_test, y_train, y_test are the same as above\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "for e in X_train:\n",
    "    x_train.append(e)\n",
    "\n",
    "for e in X_test:\n",
    "    x_test.append(e)\n",
    "\n",
    "for e in y_train:\n",
    "    Y_train.append(e)\n",
    "\n",
    "for e in y_test:\n",
    "    Y_test.append(e)\n",
    "\n",
    "# training naive Bayes model \n",
    "prior, cond = train_NB_model(x_train, Y_train)\n",
    "\n",
    "# evaluate on test set\n",
    "y_pred, prob = predict_label(x_test, prior, cond)\n",
    "\n",
    "s = []\n",
    "for i in range(len(prob)):\n",
    "    s.append(prob[i][y_pred[i]])\n",
    "\n",
    "fpr, tpr, t = roc_curve(Y_test, s)\n",
    "\n",
    "plot_roc_curve(fpr, tpr)\n",
    "\n",
    "print(roc_auc_score(Y_test,s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4)**\n",
    "\n",
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49109521 -0.40242644 -0.92156595 -0.1052653 ]\n",
      " [ 0.90298151  1.37532553  1.3897809   1.27398003]\n",
      " [-1.39407672 -0.97289909 -0.46821496 -1.16871473]]\n",
      "[0.99 3.12 4.47 4.51]\n",
      "[0.63124216 2.26128282 1.34553583 3.70492465]\n",
      "[[ 3.00e-01  1.21e+00  6.15e-01  4.02e+01]\n",
      " [ 5.60e-01  5.23e+00  2.17e+00  9.13e+01]\n",
      " [-8.90e-01 -8.00e-02  9.20e-01  8.00e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "  Normalize each feature for the input set\n",
    "  Input:\n",
    "  - X: a 2-D numpy array of shape (num_train, num_features)\n",
    "  Outputs:\n",
    "  - X_normalized: a 2-D numpy array of shape (num_train, num_features)\n",
    "  - X_mean: a 1-D numpy array of length (num_features)\n",
    "  - X_std: a 1-D numpy array of length (num_features)\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    num_train = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    X_mean = np.zeros(num_features)\n",
    "    X_std = np.zeros(num_features)\n",
    "    \n",
    "    X_normalized = np.zeros((num_train, num_features))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        f = []\n",
    "        for j in range(num_train):\n",
    "            f.append(X[j][i])\n",
    "        \n",
    "        X_mean[i] = np.mean(f)\n",
    "        X_std[i] = np.std(f)\n",
    "        \n",
    "        for j in range(num_train):\n",
    "            X_normalized[j][i] = (f[j] - X_mean[i])/X_std[i]\n",
    "        \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "    return X_normalized, X_mean, X_std\n",
    "\n",
    "\"\"\"\n",
    " Unit test case\n",
    " Should print\n",
    "[[ 0.49109521 -0.40242644 -0.92156595 -0.1052653 ]\n",
    " [ 0.90298151  1.37532553  1.3897809   1.27398003]\n",
    " [-1.39407672 -0.97289909 -0.46821496 -1.16871473]]\n",
    "[0.99 3.12 4.47 4.51]\n",
    "[0.63124216 2.26128282 1.34553583 3.70492465]\n",
    "\"\"\"\n",
    "X = np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n",
    "X_normalized, X_mean, X_std = featureNormalization(X)\n",
    "print(X_normalized)\n",
    "print(X_mean)\n",
    "print(X_std)\n",
    "\n",
    "def applyNormalization(X, X_mean, X_std):\n",
    "    \"\"\"\n",
    "  Normalize each feature for the input set X\n",
    "  Input:\n",
    "  - X: a 2-D numpy array of shape (num_test, num_features)\n",
    "  - X_mean: a 1-D numpy array of length (num_features)\n",
    "  - X_std: a 1-D numpy array of length (num_features)\n",
    "\n",
    "  Output:\n",
    "  - X_normalized: a 2-D numpy array of shape (num_test, num_features)  \n",
    "  \"\"\"\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    num_train = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    X_normalized = np.zeros((num_train, num_features))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        for j in range(num_train):\n",
    "            X_normalized[j][i] = (X[j][i] - X_mean[i])/X_std[i]\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return X_normalized\n",
    "\n",
    "\"\"\"\n",
    "Unit test case\n",
    "Should print\n",
    "[[ 3.00e-01  1.21e+00  6.15e-01  4.02e+01]\n",
    " [ 5.60e-01  5.23e+00  2.17e+00  9.13e+01]\n",
    " [-8.90e-01 -8.00e-02  9.20e-01  8.00e-01]]\n",
    "\"\"\"\n",
    "X =  np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n",
    "X_mean = np.array([1.0, 1.0, 2.0, 0.1])\n",
    "X_std = np.array([1.0, 1.0, 2.0, 0.1])\n",
    "X_normalized = applyNormalization(X, X_mean, X_std)\n",
    "print(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** in the notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0\n"
     ]
    }
   ],
   "source": [
    "def computeMSE(X, y, theta):\n",
    "    \"\"\"\n",
    "  Compute MSE for the input set (X,y) with theta\n",
    "  Inputs:\n",
    "  - X: a 2-D numpy array of shape (num_samples, num_features+1)\n",
    "  - y: a 1-D numpy array of length (num_samples)\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  Output:\n",
    "  - error: MSE, a real number\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        s = 0\n",
    "        for j in range(num_features):\n",
    "            s+=X[i][j]*theta[j][0]\n",
    "            \n",
    "#         print(s, y[i])\n",
    "        error += (s - y[i])**2\n",
    "        \n",
    "    error/=(2*num_samples)\n",
    "    \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "    return error\n",
    "\n",
    "# Unit test case:\n",
    "# Should print 73.0\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n",
    "y =  np.array([1.0, 1.0])\n",
    "theta = np.array([[1.0], [2.0],[1.0]])\n",
    "error = computeMSE(X, y, theta)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n",
      "[[ 0.76470588]\n",
      " [-0.17647059]\n",
      " [-0.11764706]]\n"
     ]
    }
   ],
   "source": [
    "# First load the data (this code block is not for grading)\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.dropbox.com/s/0ocbatan8pgm59i/winequality-white.csv?dl=1'\n",
    "file_name = 'winequality-white.csv'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "data = np.genfromtxt(file_name, delimiter=\";\", skip_header=1)\n",
    "print(data.shape)\n",
    "\n",
    "def closeForm(X, y):\n",
    "    \"\"\"\n",
    "  Compute close form solution for theta\n",
    "  Inputs:\n",
    "  - X: a numpy array of shape (num_train, num_features+1)\n",
    "  - y: a 1-D numpy array of length (num_train)\n",
    "  Output:\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  \"\"\"\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    Xt = np.matrix.transpose(X)\n",
    "    t = np.matmul(np.matmul(np.linalg.inv(np.matmul(Xt,X)),Xt),y)\n",
    "    theta = []\n",
    "    for e in t:\n",
    "        theta.append([e])\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return np.array(theta)\n",
    "\n",
    "\"\"\"\n",
    "# Unit test case:\n",
    "# Should return\n",
    "[[ 0.76470588]\n",
    " [-0.17647059]\n",
    " [-0.11764706]]\n",
    "\"\"\"\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0], [-1, 0, 2.0]])\n",
    "y = np.array([1.0, 1.0, -1.0])\n",
    "theta = closeForm(X, y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**\n",
    "\n",
    "if need to normalize the whole dataset with y answer below, if not, then please check the one below it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE using close form solution is :  0.30798526782057256\n",
      "[[ 0.03743023]\n",
      " [-0.17535703]\n",
      " [-0.00470849]\n",
      " [ 0.40417686]\n",
      " [-0.00117138]\n",
      " [ 0.08059312]\n",
      " [-0.00937812]\n",
      " [-0.44099327]\n",
      " [ 0.11382496]\n",
      " [ 0.08581885]\n",
      " [ 0.27237651]]\n",
      "test MSE using close form solution is :  16.624083988802393\n"
     ]
    }
   ],
   "source": [
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "***\n",
    "if need to normalize the whole dataset with y answer below, if not, then please check the one below it\n",
    "***\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X = data\n",
    "\n",
    "n = 3918\n",
    "\n",
    "X_test = X[n:]\n",
    "X_train = X[0:n] # approx 80% to train\n",
    "\n",
    "X_train_normalized, X_mean, X_std = featureNormalization(X_train)\n",
    "X_test_normalized = applyNormalization(X_test, X_mean, X_std)\n",
    "\n",
    "y_train = X_train_normalized[0:,11]\n",
    "y_test = X_test_normalized[0:,11]\n",
    "\n",
    "theta = closeForm(X_train[0:n,0:11], y_train)\n",
    "\n",
    "test_error = computeMSE(X_test[0:,0:11], y_test, theta)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print('test MSE using close form solution is : ', test_error)\n",
    "\n",
    "\n",
    "#_______________________If not to normalize the y________________________________________________-\n",
    "\n",
    "X = data\n",
    "\n",
    "n = 3918 # approx 80% to train\n",
    "\n",
    "X_test = X[n:,0:11]\n",
    "X_train = X[0:n,0:11] \n",
    "\n",
    "y_test = X[n:,11]\n",
    "y_train = X[0:n,11]\n",
    "\n",
    "X_train_normalized, X_mean, X_std = featureNormalization(X_train)\n",
    "X_test_normalized = applyNormalization(X_test, X_mean, X_std)\n",
    "\n",
    "theta = closeForm(X_train_normalized, y_train)\n",
    "\n",
    "print(theta)\n",
    "\n",
    "test_error = computeMSE(X_test_normalized, y_test, theta)\n",
    "\n",
    "print('test MSE using close form solution is : ', test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
