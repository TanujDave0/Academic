{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJoS8gz6ZXfV"
   },
   "source": [
    "# CS 412 Homework 2, Problem 4: Linear regression\n",
    "\n",
    "In this section, we will explore linear regression model. The dataset we will use for this section is Wine Qualuty, whose description can be found [here](http://archive.ics.uci.edu/ml/datasets/Wine+Quality). This dataset contains **4898** examples, each containing **11** features (the first 11 columns), and the **last** (12-th) column is the value we want to predict. The dataset can be downloaded here [`winequality-white.csv`](https://www.dropbox.com/s/0ocbatan8pgm59i/winequality-white.csv?dl=1) (our code will download it directly). \n",
    "\n",
    "Different from classification models, a regression model is used to predict real values rather than the category an example belongs to. Linear regression is a linear approach to modeling the relationship between features and real value target. To perform supervised learning, we represent the hypothesis as a linear function of features ($x$) to predict the output ($y$).\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n    \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\theta_i$'s are the **parameters** parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. Our goal is to **learn** these parameters so that we can find a linear function in this hypothesis space to estimate the output $y$.\n",
    "\n",
    "To simplify the notation and ease the computation, we **pad** the input $x$ by letting $x_0=1$. That is, for an example with three features $x=[x_1, x_2, x_3]$, the padded feature vector will be $x=[1,x_1, x_2, x_3]$. Then, the linear function can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "f_\\theta(x) = \\sum_{j=0}^n \\theta_j x_j = \\theta^\\top x    \\tag{9}\n",
    "\\end{equation}\n",
    "where on the right-hand side above we are viewing $\\theta$ and $x$ both as vectors, and here $n = 11$ is the number of features. \n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Data preprocessing {-}\n",
    "\n",
    "Once we have received the dataset, we first need to preprocess it.  Very often, the features in a dataset are of very different scale, which can slow down the optimization for Eq (10). To accelerate it, we need to normalize each feature by substracting its mean value, and then dividing by its standard deviation (std). Assuming $X_i = [x_i^{(1)}, ... , x_i^{(m)}]$ is the $i$-th feature in the training set (across the $m$ examples), the normalized feature $i$ for the $j$-th training example can be computed by:\n",
    "\\begin{equation}\n",
    "\\hat{x}^j_i = \\frac{x^j_i - m_i}{s_i},\n",
    "\\text{ where } m_i = mean(X_i), \\text{ and } s_i = std(X_i).\n",
    "  \\tag{11}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ3aIqTn4QSt"
   },
   "source": [
    "**Step 1: normalize the training set ** {-}\n",
    "\n",
    "In the following code block, implement a function `featureNormalization`. The input is the training set. The output is the normalized training set, along with the mean and std of each features. You will need the mean and std to apply to the test set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eyuuNjr8KITJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49109521 -0.40242644 -0.92156595 -0.1052653 ]\n",
      " [ 0.90298151  1.37532553  1.3897809   1.27398003]\n",
      " [-1.39407672 -0.97289909 -0.46821496 -1.16871473]]\n",
      "[0.99 3.12 4.47 4.51]\n",
      "[0.63124216 2.26128282 1.34553583 3.70492465]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def featureNormalization(X):\n",
    "    \"\"\"\n",
    "  Normalize each feature for the input set\n",
    "  Input:\n",
    "  - X: a 2-D numpy array of shape (num_train, num_features)\n",
    "  Outputs:\n",
    "  - X_normalized: a 2-D numpy array of shape (num_train, num_features)\n",
    "  - X_mean: a 1-D numpy array of length (num_features)\n",
    "  - X_std: a 1-D numpy array of length (num_features)\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    num_train = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    X_mean = np.zeros(num_features)\n",
    "    X_std = np.zeros(num_features)\n",
    "    \n",
    "    X_normalized = np.zeros((num_train, num_features))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        f = []\n",
    "        for j in range(num_train):\n",
    "            f.append(X[j][i])\n",
    "        \n",
    "        X_mean[i] = np.mean(f)\n",
    "        X_std[i] = np.std(f)\n",
    "        \n",
    "        for j in range(num_train):\n",
    "            X_normalized[j][i] = (f[j] - X_mean[i])/X_std[i]\n",
    "        \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "    return X_normalized, X_mean, X_std\n",
    "\n",
    "\"\"\"\n",
    " Unit test case\n",
    " Should print\n",
    "[[ 0.49109521 -0.40242644 -0.92156595 -0.1052653 ]\n",
    " [ 0.90298151  1.37532553  1.3897809   1.27398003]\n",
    " [-1.39407672 -0.97289909 -0.46821496 -1.16871473]]\n",
    "[0.99 3.12 4.47 4.51]\n",
    "[0.63124216 2.26128282 1.34553583 3.70492465]\n",
    "\"\"\"\n",
    "X = np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n",
    "X_normalized, X_mean, X_std = featureNormalization(X)\n",
    "print(X_normalized)\n",
    "print(X_mean)\n",
    "print(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op0kAryrPYgU"
   },
   "source": [
    "**Step 2: normalize the test set ** {-}\n",
    "\n",
    "The above normalization function will be used for the training set. At test time, we will need to normalize the test data in the same way. However, we shouldn't compute new mean and std from the test set itself, because it may be inconsistent with the training data.  Instead, we will apply the mean $m_i$ and std $s_i$ computed from the training set.  Given a text example $[x_1, \\ldots, x_m]$, we just transform $x_i$ into $(x_i - m_i)/s_i$,\n",
    "where $m_i$ and $s_i$ are computed from the training data as in the *where* clause of Eq (11).\n",
    "\n",
    "In the following code block, implement a function `applyNormalization`, which normalizes the test set for each feature using the provided mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NWuTPf6yTSaC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.00e-01  1.21e+00  6.15e-01  4.02e+01]\n",
      " [ 5.60e-01  5.23e+00  2.17e+00  9.13e+01]\n",
      " [-8.90e-01 -8.00e-02  9.20e-01  8.00e-01]]\n"
     ]
    }
   ],
   "source": [
    "def applyNormalization(X, X_mean, X_std):\n",
    "    \"\"\"\n",
    "  Normalize each feature for the input set X\n",
    "  Input:\n",
    "  - X: a 2-D numpy array of shape (num_test, num_features)\n",
    "  - X_mean: a 1-D numpy array of length (num_features)\n",
    "  - X_std: a 1-D numpy array of length (num_features)\n",
    "\n",
    "  Output:\n",
    "  - X_normalized: a 2-D numpy array of shape (num_test, num_features)  \n",
    "  \"\"\"\n",
    "\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    num_train = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    X_normalized = np.zeros((num_train, num_features))\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        for j in range(num_train):\n",
    "            X_normalized[j][i] = (X[j][i] - X_mean[i])/X_std[i]\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    return X_normalized\n",
    "\n",
    "\"\"\"\n",
    "Unit test case\n",
    "Should print\n",
    "[[ 3.00e-01  1.21e+00  6.15e-01  4.02e+01]\n",
    " [ 5.60e-01  5.23e+00  2.17e+00  9.13e+01]\n",
    " [-8.90e-01 -8.00e-02  9.20e-01  8.00e-01]]\n",
    "\"\"\"\n",
    "X =  np.array([[1.30,2.21,3.23,4.12], [1.56, 6.23, 6.34, 9.23], [0.11, 0.92, 3.84, 0.18]])\n",
    "X_mean = np.array([1.0, 1.0, 2.0, 0.1])\n",
    "X_std = np.array([1.0, 1.0, 2.0, 0.1])\n",
    "X_normalized = applyNormalization(X, X_mean, X_std)\n",
    "print(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP3V48YIt_yo"
   },
   "source": [
    "**Implement the loss function** {-} \n",
    "Given a training set, the way to learn these parameters is to make $f_\\theta(x)$ close to $y$. To measure the closeness, we use Mean-Squared-Error (MSE) here. The loss function can therefore be defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(f_\\theta(x^{(i)})-y^{(i)})^2 = \\frac{1}{2m}\\sum_{i=1}^m(\\theta^\\top x^{(i)}-y^{(i)})^2,  \\tag{10}\n",
    "\\end{equation}\n",
    "where the superscript $(i)$ denotes the $i$-th example, \n",
    "and $m$ is the total number of training samples. To learn the parameter $\\theta$, our goal is to **minimize** the above loss function. Here, we will explore exact method to learn the parameter: Closed-form solution (root of the gradient).\n",
    "\n",
    "We will use MSE to measure the quality of our parameters, also known as the loss. In the following code block, implement a function `computeMSE`. Follow Equation (10), and the function should compute the MSE for the input set with the given $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "o_I9hUncTPMU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.0\n"
     ]
    }
   ],
   "source": [
    "def computeMSE(X, y, theta):\n",
    "    \"\"\"\n",
    "  Compute MSE for the input set (X,y) with theta\n",
    "  Inputs:\n",
    "  - X: a 2-D numpy array of shape (num_samples, num_features+1)\n",
    "  - y: a 1-D numpy array of length (num_samples)\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  Output:\n",
    "  - error: MSE, a real number\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    num_samples = X.shape[0]\n",
    "    num_features = X.shape[1]\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        s = 0\n",
    "        for j in range(num_features):\n",
    "            s+=X[i][j]*theta[j][0]\n",
    "            \n",
    "#         print(s, y[i])\n",
    "        error += (s - y[i])**2\n",
    "        \n",
    "    error/=(2*num_samples)\n",
    "    \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "  \n",
    "    return error\n",
    "\n",
    "# Unit test case:\n",
    "# Should print 73.0\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\n",
    "y =  np.array([1.0, 1.0])\n",
    "theta = np.array([[1.0], [2.0],[1.0]])\n",
    "error = computeMSE(X, y, theta)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "0kiPKQ9XHDfg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "# First load the data (this code block is not for grading)\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.dropbox.com/s/0ocbatan8pgm59i/winequality-white.csv?dl=1'\n",
    "file_name = 'winequality-white.csv'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "data = np.genfromtxt(file_name, delimiter=\";\", skip_header=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2Ox89manO-n"
   },
   "source": [
    "## 2.5 Closed-form solution  {-}\n",
    "\n",
    "We can find the $\\theta$ explicitly by finding the root of the gradient equation $\\nabla_\\theta L(\\theta)=0$ (i.e., the $\\theta$ such that $\\nabla_\\theta L(\\theta) = 0$), we can obtain a closed-form solution of $\\theta$ that minimizes the loss $L$ by algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw9FrxOB7KE_"
   },
   "source": [
    "**Step 1: find the root of the gradient equation to obtain the closed-form solution of $\\theta$ **\n",
    "\n",
    "Type your result in the following lines:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}L(\\theta) = 0 \\quad \\Rightarrow \\quad \\theta = (X^\\top X) ^{-1} X^\\top y \n",
    "$$\n",
    "Then, implement a function `closeForm` to compute the closed-form solution of $\\theta$ using the expression you have derived above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "HU3ltzQ1r9U3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.76470588]\n",
      " [-0.17647059]\n",
      " [-0.11764706]]\n"
     ]
    }
   ],
   "source": [
    "def closeForm(X, y):\n",
    "    \"\"\"\n",
    "  Compute close form solution for theta\n",
    "  Inputs:\n",
    "  - X: a numpy array of shape (num_train, num_features+1)\n",
    "  - y: a 1-D numpy array of length (num_train)\n",
    "  Output:\n",
    "  - theta: a 1-D numpy array of length (num_features+1)\n",
    "  \"\"\"\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    Xt = np.matrix.transpose(X)\n",
    "    t = np.matmul(np.matmul(np.linalg.inv(np.matmul(Xt,X)),Xt),y)\n",
    "    theta = []\n",
    "    for e in t:\n",
    "        theta.append([e])\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return np.array(theta)\n",
    "\n",
    "\"\"\"\n",
    "# Unit test case:\n",
    "# Should return\n",
    "[[ 0.76470588]\n",
    " [-0.17647059]\n",
    " [-0.11764706]]\n",
    "\"\"\"\n",
    "X = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0], [-1, 0, 2.0]])\n",
    "y = np.array([1.0, 1.0, -1.0])\n",
    "theta = closeForm(X, y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUIWxmR0sY4J"
   },
   "source": [
    "**Step 2: evaluate the test error using closed-form solution ** {-} \n",
    "\n",
    "Evaluate the new $\\theta$ on test set by **printing** the test error (MSE) in the format: \"test MSE using close form solution is : __\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "iHtEAKdXtRoD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE using close form solution is :  0.30798526782057256\n",
      "test MSE using close form solution is :  16.624083988802393\n"
     ]
    }
   ],
   "source": [
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "if need to normalize the whole dataset with y answer below, if not, then please check the one below it\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "X = data\n",
    "\n",
    "n = 3918\n",
    "\n",
    "X_test = X[n:]\n",
    "X_train = X[0:n] # approx 80% to train\n",
    "\n",
    "X_train_normalized, X_mean, X_std = featureNormalization(X_train)\n",
    "X_test_normalized = applyNormalization(X_test, X_mean, X_std)\n",
    "\n",
    "y_train = X_train_normalized[0:,11]\n",
    "y_test = X_test_normalized[0:,11]\n",
    "\n",
    "theta = closeForm(X_train[0:n,0:11], y_train)\n",
    "\n",
    "test_error = computeMSE(X_test[0:,0:11], y_test, theta)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print('test MSE using close form solution is : ', test_error)\n",
    "\n",
    "\n",
    "#_______________________If not to normalize the y________________________________________________-\n",
    "\n",
    "X = data\n",
    "\n",
    "n = 3918 # approx 80% to train\n",
    "\n",
    "X_test = X[n:,0:11]\n",
    "X_train = X[0:n,0:11] \n",
    "\n",
    "y_test = X[n:,11]\n",
    "y_train = X[0:n,11]\n",
    "\n",
    "X_train_normalized, X_mean, X_std = featureNormalization(X_train)\n",
    "X_test_normalized = applyNormalization(X_test, X_mean, X_std)\n",
    "\n",
    "theta = closeForm(X_train_normalized, y_train)\n",
    "\n",
    "test_error = computeMSE(X_test_normalized, y_test, theta)\n",
    "\n",
    "print('test MSE using close form solution is : ', test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
