{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-C8omlh6Pq4"
   },
   "source": [
    "# **CS 412 Homework 2, Problem 3: Naive Bayes Classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuqxFfyO6Pq6"
   },
   "source": [
    "## Problem 1. Naive Bayes Classification  {-}\n",
    "\n",
    "In this problem, you will implement the Naive Bayes classification method and use it for SMS message classifcation. The SMS dataset `SMSSpamCollection` has been provided in the assignment folder, which also can be downloaded from the [UCI Link](http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).  In case the repository gets offline occasionally, I have provided the files on Blackboard.\n",
    "\n",
    "To help you to better understand this algorithm, you are **not** allowed to use any off-the-shelf naive Bayes implementations from third-party libraries. We will implement it with detailed step-by-step instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "io7R6hOn6Pq9"
   },
   "source": [
    "### Recap of the Naive Bayes Algorithm  {-}\n",
    "Naive Bayes classification is a fast and simple classification method. Its efficiency stems from some simplifications we make about the underlying probability distributions, namely, the assumption about the conditional independence of features. Suppose for any class $Y$, we have a probability distribution over all possible combinations of values for a feature vector $X$:\n",
    "$$\n",
    "P(X|Y).\n",
    "$$\n",
    "The main idea of Bayesian classification is to reverse the direction of dependence --- we want to predict the label based on the features:\n",
    "$$\n",
    "P(Y|X).\n",
    "$$\n",
    "This is made possible by the Bayes theorem:\n",
    "\\begin{equation}\n",
    "P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}. \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "To make it more concrete, let us consider the SMS message classification problem. Ignoring punctuations, each SMS message contains an ordered sequence of $T$ words (case-insensitive) $X = \\{X_1, ...,X_T\\}$. That is, $X$ corresponds to an SMS message, and $X_i$ corresponds to the $i$-th word in it. For each message from the training set, there is a corresponding label $Y\\in\\{spam, ham\\}$. \n",
    "\n",
    "**Model specification and key assumption.** The conditional distribution can be written as:\n",
    "$$\n",
    "P(X|Y) = P(X_1, ..., X_T|Y).\n",
    "$$\n",
    "Since this conditional probability is intractable, we simplify it in two steps:\n",
    "\n",
    "1. **Assume** that all features $X_i$ are independent, conditional on the category $Y$. This leads to a naive Bayes model which writes formally as\n",
    "\\begin{equation}\n",
    "P(X|Y) = P(X_1, ..., X_T|Y) = \\prod_{i=1}^T P(X_i|Y). \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "2. **Assume** that $P(X_i | Y) = P(X_j|Y)$ for all $i \\neq j$.\n",
    "In other words, given the label $Y$, the value of the $7$-th word has the same distribution as the value of the $10$-th word. Note this is not assumed by default in naive Bayes, and we make this additional assumption to significantly simplify our model.\n",
    "It is often referred to as \"tying\" the probability $P(X_i|Y)$ over $i$.\n",
    "As a result, the order of words no longer matters for $P(X|Y)$,\n",
    "i.e., \n",
    "$$P(X=\\text{'cat is cute'}|Y) \\ \\ = \\ \\ P(X=\\text{'cute cat is'}|Y) \\quad \\text{for all } Y.\n",
    "$$\n",
    "\n",
    "Plugging Eq (2) into the Bayes theorem in Eq (1), we arrive at\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(Y|X) &= \\frac{P(Y) P(X|Y)}{P(X)} = \\frac{P(Y)\\prod_{i=1}^T P(X_i|Y)}{P(X)} \\\\\n",
    "&\\propto P(Y)\\prod_{i=1}^T P(X_i|Y),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\propto$ denotes proportionality. Since the denominator $P(X)$ does not depend on $Y$, the prediction probability is proportional to the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWSd8rJUqhZ7"
   },
   "source": [
    "**Making predictions.**\n",
    "Naturally, given an SMS message $X$, we can first compute $P(Y|X)$ for all possible categories $Y$ (in this example, only two categories), and then make predictions by outputting the $Y$ that maximizes the probability. This can be expressed mathmatically as:\n",
    "$$\n",
    "\\arg\\max_Y P(Y)\\prod_{i=1}^T P(X_i|Y) = \\arg\\max_Y \\left\\{\\log P(Y) + \\sum_{i=1}^T \\log P(X_i|Y) \\right\\}. \\tag{3}\n",
    "$$\n",
    "If there is a tie, we just break it arbitrarily.\n",
    "Here the logarithm uses natural basis.\n",
    "\n",
    "**Learning the model.**\n",
    "To apply the prediction rule in Eq (3), we need to first figure out (formally termed \"estimate\") the value of $P(Y)$ and $P(X_i|Y)$ by using the training data. Recall that since we tie the conditional probabilities $P(X_i|Y)$ across all $i$,\n",
    "the subscript $i$ can be dropped *(Why?)*.\n",
    "However, we still carry it just for clarity. \n",
    "\n",
    "Firstly, $P(Y=y)$ can be estimated by computing the frequency of category $y$ in the whole training set ($y$ can be either \"$spam$\" or \"$ham$\"). Here, in order to avoid confusion, we have used the convention that capital letters denote random variables, and lowercase letters denote their possible instantiations.\n",
    "\n",
    "Secondly, $P(X_i = w |Y=y)$ for a word $w$ (e.g., \"cat\") can be estimated by counting the frequency that it appears in the training message set for a given category $y$:\n",
    "\\begin{align}\n",
    "P(X_i &= w|Y=y) \n",
    "= \\frac{Count(w, y)}{Count(y)}, \\ \\ where\n",
    "\\tag{4} \\\\\n",
    "\\notag\n",
    " Count(w, y) &= \\text{total number of occurrence of $w$ in all SMS messages of category } y \\\\\n",
    "\\notag\n",
    "Count(y) &= \\text{total number of words appearing in SMS messages of category } y.\n",
    "\\end{align}\n",
    "\n",
    "*Remark 1:* If $w$ appears in a single message for 3 times, then it contributes to $Count(w, y)$ by 3, not 1. Similarly, $Count(y)$ indeed equals the total length of all messages in category $y$.\n",
    "\n",
    "For example, suppose there are four messages \n",
    "$$\n",
    "\\text{{'cat is cute', ham}, {'dog rocks', spam}, {'whatever is is right', ham}, {'hello', spam}.}\n",
    "$$\n",
    "Then $P(X_i = '\\text{is}' | \\text{ham}) = 3 / 7$ (**not** $2/7$),\n",
    "and $P(X_i = '\\text{is}' | \\text{spam}) = 0 / 3$.\n",
    "\n",
    "Quiz (no need to submit): compute $P(X_i = w | \\text{ham})$, for $w = $ 'cat', 'is', 'cute', 'whatever', and 'right'. Check if their sum is 1. In essence, a word appearing for multiple times in a single message should be counted multiple times.\n",
    "\n",
    "*Remark 2:* Obviously, the right-hand side of Eq (4) does not depend on $i$. This is consistent with our previous note that we carry the subscript $i$ in $P(X_i|Y)$ only for clarity, while in fact different $i$ share the same $P(X_i|Y)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You may have noticed that any word $w$ with $Count(w,y)=0$ leads to $P(X_i = w|Y=y) = 0$.\n",
    "As a result, by Eq (2), any message $x$ has conditional probably $P(X = x |Y=y) = 0$ if $w$ appears in $x$.\n",
    "Such a \"veto\" is not favorable, and can create significant problems when a word in the test data has never appeared in the training data (think why?).\n",
    "To bypass this issue, we can add pseudo-count, a.k.a additive smoothing:\n",
    "$$\n",
    "\\hat{P}(X_i = w|Y=y) = \\frac{Count(w, y) + \\alpha}{Count(y) + N\\alpha}, \\tag{5}\n",
    "$$\n",
    "where $\\alpha$ is a smoothing parameter. $\\alpha=0$ corresponds to no smoothing. In our experiment, let us set $\\alpha = 1.0$. $N$ denotes the number of distinct words in the vocabulary, and we will set $N = 20,000$.\n",
    "\n",
    "Now Let's start with data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TqrY7MaJ6PrA"
   },
   "outputs": [],
   "source": [
    "# set up code for this experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VYKq9-6PrP"
   },
   "source": [
    "### Data Preprocessing (not for grading) {-}\n",
    "\n",
    "We will use `pandas` to import the dataset. Since `SMSSpamCollection` separates labels and text content in each message by a tab, we will use '\\t' as the value for the `sep` argument and read raw data into a pandas dataframe. As a result, we store labels and SMS messages into two columns. To facilitate the subsequent steps, we also rename the columns by passing a list `['label', 'sms_message']` to the `names` argument of the `read_table()` method.\n",
    "\n",
    "Let us print the first five rows of the dataframe to get a basic understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FXBuWAZ06PrQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset to the server\n",
    "# Import the data using the read_csv() method from pandas\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.dropbox.com/s/ybnfa73k41loqbx/SMSSpamCollection.dat.txt?dl=1'\n",
    "file_name = 'SMSSpamCollection.dat'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "df = pd.read_csv(file_name,\n",
    "                    sep='\\t',\n",
    "                    header=None,\n",
    "                    names=['label', 'sms_message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzzbwM0h6Prc"
   },
   "source": [
    "#### Step 1: Convert string labels to numerical labels (not for grading) {-}\n",
    "\n",
    "As we can see, there are 2 columns. The first column, which is named `label`, takes two values `spam` (the message is spam) and `ham` (the message is not spam). The second column is the text content of the SMS message that is being classified.  It is a string in which words are separated by space.\n",
    "\n",
    "Note that the string-typed labels are unwieldy for calculating performance metrices, e.g., when calculating precision and recall scores. Hence, let's convert the labels to binary variables, 0 for `ham` and 1 for `spam`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "U5myRuvh6Prd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the next line only once after running the previous code block\n",
    "# Running it more than once will turn the labels into NaN\n",
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Qsp7_C6Prg"
   },
   "source": [
    "#### Step 2: Bag of words  {-}\n",
    "\n",
    "What we have in our dataset is a large collection of text data (5,572 rows/messages). Most ML algorithms rely on numerical data to be fed into them as input, but SMS messages are usually text heavy. \n",
    "\n",
    "To address this issue, we would like to introduce the concept of Bag of Words (BoW), which is designed for problems with a 'bag of words' or a collection of text data. The basic idea is to count the frequency of the words in the text. It is important to note that BoW treats each word individually, ignoring the order in which the words occur. \n",
    "\n",
    "To count the frequency of the words in text, usually we need to process the input text data in four steps:\n",
    "\n",
    "- Convert all strings into their lower case form\n",
    "- Removing all punctuations\n",
    "- Tokenization, i.e., split a sentence into individual words\n",
    "- Count frequencies\n",
    "\n",
    "Once this has been done, we are supposed to obtain a vocabulary dictionary with frequencies of each words for the given text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Gf3M2dHK6Pri",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1, 'me': 1, 'now': 1, 'tomorrow': 1})\n"
     ]
    }
   ],
   "source": [
    "def count_frequency(documents):\n",
    "    \"\"\"\n",
    "    count occurrence of each word in the document set.\n",
    "    Inputs:\n",
    "    - documents: list, each entity is a string type SMS message\n",
    "    Outputs:\n",
    "    - frequency: a dictionary. The key is the unique words, and the value is the number of occurrences of the word\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Here is the pseudo-code\n",
    "    # Step 1: covert all strings into their lower case form\n",
    "    lower_case_doc = []\n",
    "    for s in documents:\n",
    "        lower_case_doc.append(???)\n",
    "    \n",
    "    # Step 2: remove all punctuations\n",
    "    no_punc_doc = []\n",
    "    for s in lower_case_doc:\n",
    "        no_punc_doc.append(???)\n",
    "    \n",
    "    # Step 3: tokenize a sentence, i.e., split a sentence into individual words \n",
    "    # using a delimiter. The delimiter specifies what character we will use to identify the beginning \n",
    "    # and the end of a word.\n",
    "    words_doc = []\n",
    "    for s in no_punc_doc:\n",
    "        words_doc.append(???)\n",
    "    \n",
    "    # Step 4: count frequencies. To count the occurrence of each word in the document set. \n",
    "    # We can use the `Counter` method from the Python `collections` library for this purpose. \n",
    "    # `Counter` counts the occurrence of each item in the list and returns a dictionary with \n",
    "    # the key as the item being counted and the corresponding value being the count of that item in the list. \n",
    "    all_words = []\n",
    "    for s in words_doc:\n",
    "        all_words.extend(???)\n",
    "    frequency = \"some function/constructor on all_words\"\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # Step 1: covert all strings into their lower case form\n",
    "    lower_case_doc = []\n",
    "    for s in documents:\n",
    "        lower_case_doc.append(s.lower())\n",
    "    \n",
    "    # Step 2: remove all punctuations\n",
    "    no_punc_doc = []\n",
    "    for s in lower_case_doc:\n",
    "        no_punc_doc.append(s.translate(str.maketrans('','', string.punctuation)))\n",
    "    \n",
    "    # Step 3: tokenize a sentence, i.e., split a sentence into individual words \n",
    "    # using a delimiter. The delimiter specifies what character we will use to identify the beginning \n",
    "    # and the end of a word.\n",
    "    words_doc = []\n",
    "    for s in no_punc_doc:\n",
    "        words_doc.append(s.split())\n",
    "    \n",
    "    # Step 4: count frequencies. To count the occurrence of each word in the document set. \n",
    "    # We can use the `Counter` method from the Python `collections` library for this purpose. \n",
    "    # `Counter` counts the occurrence of each item in the list and returns a dictionary with \n",
    "    # the key as the item being counted and the corresponding value being the count of that item in the list. \n",
    "    all_words = []\n",
    "    for s in words_doc:\n",
    "        all_words.extend(s)\n",
    "    frequency = Counter(all_words)\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return frequency\n",
    "\n",
    "# Unit test case:\n",
    "# documents = ['Hello, how are you!', \n",
    "#              'Win money, win from home.',\n",
    "#              'Call me now.',\n",
    "#              'Hello, Call hello you tomorrow?']\n",
    "# sample outputs:\n",
    "# Counter({'hello': 3, 'you': 2, 'win': 2, 'call': 2, 'how': 1, 'are': 1, 'money': 1, 'from': 1, 'home': 1,\n",
    "# 'me': 1, 'now': 1, 'tomorrow': 1})\n",
    "documents = ['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?']\n",
    "\n",
    "freq = count_frequency(documents)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ5l72qQ6Prn"
   },
   "source": [
    "#### Step 3: Create training and test sets  {-}\n",
    "\n",
    "We will partition the `SMSSpamCollection` dataset into training and test sets so that we can analyze the model's performance on data it has not witnessed during training. Here, we will learn to use the `scikit` library to do splitting for us. `scikit` is a powerfull tool for machine learning and data mining, providing plenty of well-designed methods for data analysis. We'll use its `train_test_split()` method to create training and testing sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, you need to set the `random_state` argument of `train_test_split()` to **1**.\n",
    "\n",
    "No need to do cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "KXkkpzP46Pro",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataset contains 5572 examples in total.\n",
      "The training set contains 4457 examples.\n",
      "The testing set contains 1115 examples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# learn to read API documentation\n",
    "# you can get detailed instructions about this method through this link:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], df['label'], test_size = 0.2, random_state = 1)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f'The original dataset contains {df.shape[0]} examples in total.')\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {X_test.shape[0]} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVB4Giy56Prv"
   },
   "source": [
    "### Implementing Naive Bayes method from scratch {-}\n",
    "\n",
    "#### Step 1: training the Naive Bayes Model {-}\n",
    "\n",
    "Now that we know what Naive Bayes is, we can take a closer look at how to calculate the posterior probability\n",
    "$$\n",
    "P(Y|X) \\propto P(Y)\\prod_{i=1}^T P(X_i|Y).\n",
    "$$\n",
    "\n",
    "The goal of training is to learn the prior and conditional probability from data. The calculation of the prior $P(Y=y)$ is straightforward. It can be estimated via the frequency of messages in the training set that belong to class $y$, e.g.,\n",
    "$$\n",
    "P(Y=spam) = \\frac{\\# \\text{training messages in the spam category}}{\\# \\text{training messages}}.\n",
    "$$\n",
    "\n",
    "The conditional probability given the class label --- $P(X_i|Y)$ --- can also be estimated from the data by using Eq (5). As we assumed above, it is indeed independent of $i$ (i.e., shared by all $i$). We will leave it to you to translate Eq (5) into a concrete computation scheme. No pseudo-code is provided because by now you should be able to do it. However, do make sure that your implementation complies with the input and output data types as specified in the code.\n",
    "\n",
    "**Hint**: \n",
    "- `count_frequency()` can be useful for computing the conditional probability.\n",
    "- You need to apply the **pseudo-count** trick to handle unseen words when computing the conditional probability for testing data. It is natual to ask how to carry the quantity Count(y) in Equation 5 through the variables that are passed through the functions.  To address this issue, we can create a 'unseen_words' entry in cond_prob (the variable returned by the function conditional_prob), and set its value to $\\hat{P}(X_i = unseen\\_words | Y = y) = \\alpha / (Count(y) + N \\alpha)$.  Then all the words that appear in test data but not in training data can directly use that entry.  Please do not use other key names than 'unseen_words', because the auto-grader follows this protocal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "CATiEZSw6Prw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.5}\n",
      "{0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, 'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, 'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05, 'unseen_words': 4.99775101204458e-05}, 1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, 'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, 'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, 'now': 9.996001599360256e-05, 'unseen_words': 4.998000799680128e-05}}\n"
     ]
    }
   ],
   "source": [
    "def train_NB_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    training a naive bayes model from the training data.\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Output:\n",
    "    - prior: a dictionary, whose key is the class label, and value is the prior probability.\n",
    "    - conditional: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    \"\"\"\n",
    "\n",
    "    # To make your code more readable, you can implement some auxiliary functions\n",
    "    # such as `prior_prob` and `conditional_prob` outside of this train_NB_model function\n",
    "\n",
    "    # compute the prior probability\n",
    "    prior = prior_prob(y_train)\n",
    "    \n",
    "    # compute the conditional probability\n",
    "    conditional = conditional_prob(X_train, y_train)\n",
    "\n",
    "    return prior, conditional\n",
    "\n",
    "# Start your auxiliary functions\n",
    "    \n",
    "def prior_prob(y_train):\n",
    "    \"\"\"\n",
    "    compute the prior probability\n",
    "    Inputs:\n",
    "    - y_train: an array that stores ground true label for training data\n",
    "    Outputs:\n",
    "    - prior: a dictionary. key is the class label, value is the prior probability.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    n = len(y_train)\n",
    "    \n",
    "    all_words = []\n",
    "    for s in y_train:\n",
    "        all_words.append(s)\n",
    "    p = Counter(all_words)\n",
    "    \n",
    "    prior = {}\n",
    "    \n",
    "    for e in p:\n",
    "        prior[e] = p[e]/n\n",
    "    \n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return prior\n",
    "\n",
    "def conditional_prob(X_train, y_train):\n",
    "    \"\"\"\n",
    "    compute the conditional probability for a document set\n",
    "    Inputs:\n",
    "    - X_train: an array of shape (num_train,) which stores SMS messages. each entity is a string type SMS message\n",
    "    - y_train: an array of shape (num_train,). the ground true label for each training data.\n",
    "    Ouputs:\n",
    "    - cond_prob: a dictionary. key is the class label, value is a dictionary in which the key is word, the value is the conditional probability of feature x_i given y.\n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    d = {}\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        if y_train[i] not in d:\n",
    "            d[y_train[i]] = []\n",
    "        \n",
    "        d[y_train[i]].append(X_train[i])\n",
    "    \n",
    "    cond_prob = {}\n",
    "    \n",
    "    for e in d:\n",
    "        counter = count_frequency(d[e])\n",
    "        n = 0\n",
    "        for c in counter:\n",
    "            n += counter[c]\n",
    "        f = {}\n",
    "        for c in counter:\n",
    "            f[c] = (counter[c]+1.0)/(n+20000)\n",
    "        f['unseen_words'] = 1.0/(n+20000)\n",
    "        \n",
    "        cond_prob[e] = f\n",
    "    \n",
    "    \n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****   \n",
    "    \n",
    "    return cond_prob\n",
    "\n",
    "# unit test case:\n",
    "# x_train = ['Hello, how are you!',\n",
    "#           'Win money, win from home.',\n",
    "#            'Call me now.',\n",
    "#            'Hello, Call hello you tomorrow?']\n",
    "# y_train = np.array([0,1,1,0])\n",
    "#\n",
    "# sample outputs:\n",
    "# prior: {0: 0.5, 1: 0.5}\n",
    "# conditional: {0: {'hello': 0.0001999100404817832, 'how': 9.99550202408916e-05, \n",
    "#                   'are': 9.99550202408916e-05, 'you': 0.0001499325303613374, \n",
    "#                   'call': 9.99550202408916e-05, 'tomorrow': 9.99550202408916e-05}, \n",
    "#               1: {'win': 0.00014994002399040384, 'money': 9.996001599360256e-05, \n",
    "#                   'from': 9.996001599360256e-05, 'home': 9.996001599360256e-05, \n",
    "#                   'call': 9.996001599360256e-05, 'me': 9.996001599360256e-05, \n",
    "#                   'now': 9.996001599360256e-05}}\n",
    "x_train = np.array(['Hello, how are you!',\n",
    "            'Win money, win from home.',\n",
    "            'Call me now.',\n",
    "            'Hello, Call hello you tomorrow?'])\n",
    "y_train_mini = np.array([0,1,1,0])\n",
    "\n",
    "prior = prior_prob(y_train_mini)\n",
    "cond_prob = conditional_prob(x_train, y_train_mini)\n",
    "print(prior)\n",
    "print(cond_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdPoLF0-6Pr2"
   },
   "source": [
    "#### Step 2: predict label for test data {-}\n",
    "\n",
    "Once we have the two models $P(Y)$ and $P(X_i|Y)$ from *training*, we can use them to predict the label for a given test message. To this end, we need to compute the probability of all possible labels, and then predict the one with maximum probability value:\n",
    "$$\n",
    "\\arg\\max_Y P(Y)\\prod_{i=1}^T P(X_i|Y). \\tag{6}\n",
    "$$\n",
    "\n",
    "**Avoid numerical underflow with log-trick.**\n",
    "As shown in the above equation, the calculation involves multiplying many probabilities together. Since probabilities lie in $(0,1]$, multiplying many of them together can lead to numerical underflow (i.e., a floating point number close to 0 gets rounded down to 0 by a computer), especially when $T$ is large, i.e., the test message is long.\n",
    "\n",
    "To overcome this problem, it is common to change the calculation from the product of probabilities to the sum of log probabilities. \n",
    "That is, take the natual logarithm of the right-hand side of Eq (6) as\n",
    "$$\n",
    "g_Y(X) = \\log P(Y) + \\sum_{i=1}^T \\log P(X_i|Y). \\tag{7}\n",
    "$$\n",
    "It is much more numerically stable to compute $g_Y(X)$ and to take $\\arg\\max_Y g_Y(X)$ to find the most likely class label (as the output prediction). \n",
    "\n",
    "\n",
    "Again, you are expected to implement Eq (7) and loop over all test examples by yourself with no pseudo-code given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5iDZ9QpB6Pr4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] [[0.9230591772627317, 0.07694082273726824]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "def predict_label(X_test, prior_prob, cond_prob):\n",
    "    \"\"\"\n",
    "    predict the class labels for the testing set\n",
    "    Inputs:\n",
    "    - X_test: an array of shape (num_test,) which stores test data. \n",
    "              Each entity is a string type SMS message.\n",
    "    - prior_prob: a dictionary which stores the prior probability for all categories\n",
    "              We previously used \"prior_prob\" as the name of function.  \n",
    "              Here it is used as a dictionary name.  No confusion should arise.\n",
    "    - cond_prob: a dictionary whose key is the class label y, and value is another dictionary.\n",
    "                   In the latter dictionary, the key is word w, and the value is the\n",
    "                   conditional probability P(X_i = w | y).\n",
    "    Outputs:\n",
    "    - predict: an array that stores predicted labels\n",
    "    - test_prob: an array of shape (num_test, num_classes) which stores the posterior probability of each class\n",
    "    \"\"\"\n",
    "\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    predict = []\n",
    "    test_prob = []\n",
    "    \n",
    "    for sms in X_test:\n",
    "        p = []\n",
    "        t = 0\n",
    "        \n",
    "        for label in prior_prob:\n",
    "            g = np.log(prior_prob[label])\n",
    "            am = prior_prob[label]\n",
    "            \n",
    "            s = count_frequency([sms])\n",
    "            \n",
    "            for x in s:\n",
    "                if x in cond_prob[label]:\n",
    "                    g+=np.log(cond_prob[label][x])\n",
    "                    am*=cond_prob[label][x]\n",
    "                else:\n",
    "                    g+=np.log(cond_prob[label]['unseen_words'])\n",
    "                    am*=cond_prob[label]['unseen_words']\n",
    "            t+=am\n",
    "\n",
    "            p.append(am)\n",
    "        for i in range(len(p)):\n",
    "            p[i] /= t\n",
    "        test_prob.append(p)\n",
    "        \n",
    "        predict.append(p.index(max(p)))\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return predict, test_prob\n",
    "\n",
    "\n",
    "# unit test case:\n",
    "# x_test = np.array(['Hello, how are you!'])\n",
    "# sample outputs:\n",
    "# y_pred: [0] \n",
    "# prob: [[0.92298104 0.07701896]]\n",
    "x_test = np.array(['Hi, how are you today!'])\n",
    "y_pred, test_prob = predict_label(x_test, prior, cond_prob)\n",
    "print(y_pred, test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QINYf3KB6Pr-"
   },
   "source": [
    "#### Step 3: compute performance metrics {-}\n",
    "You may have noticed that the classes are heavily imbalanced. There are only 747 `spam` messages, compared with 4827 `ham` messages. If a classifier simply predicts all messages as `ham`, it will get around 86% accuracy (pretty high). Therefore, accuracy is not a good metric in this case for evaluating the performance of the classifier. As we did before, we can use F-score metrics. But this time we will not implement it from the scratch. Instead, we will learn how to use the builtin methods from `scikit`. [Here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) is a summary of well-implemented and commonly used metrics for evaluating the quality of a model's predictions. \n",
    "\n",
    "In this task, you need to **report** the testing accuracy, confusion matrix, and F1 score of the Naive Bayes method by choosing proper functions from `scikit` to compute those metrics with required arguments.\n",
    "\n",
    "Hint: you need to import methods from `sklearn.metrics` before using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Wb58oHwb6PsA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def compute_metrics(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    compute the performance metrics\n",
    "    Inputs:\n",
    "    - y_pred: an array of predictions\n",
    "    - y_true: an array of ground true labels\n",
    "    Outputs:\n",
    "    - acc: accuracy\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    acc = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return acc\n",
    "\n",
    "# unit test case:\n",
    "# y_pred = np.array([0,1,1,1,0,1])\n",
    "# y_true = np.array([0,1,0,0,1,1])\n",
    "# \n",
    "# sample outputs:\n",
    "# acc: 0.5 \n",
    "# cm: [[1 2]\n",
    "#      [1 2]] \n",
    "# f1: 0.5714285714285715\n",
    "y_pred = np.array([0,1,1,1,0,1])\n",
    "y_true = np.array([0,1,0,0,1,1])\n",
    "acc = compute_metrics(y_pred, y_true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZLTG_uros32"
   },
   "source": [
    "#### Step 4: Plot ROC curve and print other results {-}\n",
    "\n",
    "ROC (Receiver Operating Characteristics) curve is one of the most commonly used metrics for evaluating the performance of machine learning algorithms, especially when the classes are imbalanced.\n",
    "\n",
    "ROC is a probability curve for different classes. ROC tells us how good the model is for distinguishing the given classes, in term of the **predicted probability** (not the final hard label in pos/neg). A typical ROC curve has False Positive Rate (FPR) on the $x$-axis and True Positive Rate (TPR) on the $y$-axis. To obtain the FPR and TPR, you can use the `roc_curve` method from `scikit`. This `roc_curve` function takes two arguments: 1) the ground truth labels of the test examples, and 2) the predicted probability that each example is positive. It returns the FPR and TPR which can be used for plotting.\n",
    "\n",
    "You can even compute the area under the curve (AUC) by calling `roc_auc_score` which takes the same arguments as `roc_curve` required.\n",
    "\n",
    "In this task, **plot** the ROC curve and compute the AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "6DiYOIdIo7F7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57190293472761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "  plt.plot(fpr, tpr, color='blue', label='ROC')\n",
    "  plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "  plt.xlabel('FPR')\n",
    "  plt.ylabel('TPR')\n",
    "  plt.title('ROC Curve')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# We now compute the test performance.\n",
    "# X_train, X_test, y_train, y_test are the same as above\n",
    "\n",
    "x_train = []\n",
    "x_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "\n",
    "for e in X_train:\n",
    "    x_train.append(e)\n",
    "\n",
    "for e in X_test:\n",
    "    x_test.append(e)\n",
    "\n",
    "for e in y_train:\n",
    "    Y_train.append(e)\n",
    "\n",
    "for e in y_test:\n",
    "    Y_test.append(e)\n",
    "\n",
    "# training naive Bayes model \n",
    "prior, cond = train_NB_model(x_train, Y_train)\n",
    "\n",
    "# evaluate on test set\n",
    "y_pred, prob = predict_label(x_test, prior, cond)\n",
    "\n",
    "s = []\n",
    "for i in range(len(prob)):\n",
    "    s.append(prob[i][y_pred[i]])\n",
    "\n",
    "fpr, tpr, t = roc_curve(Y_test, s)\n",
    "\n",
    "plot_roc_curve(fpr, tpr)\n",
    "\n",
    "print(roc_auc_score(Y_test,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
