{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMM2e3OP6Kn4"
   },
   "source": [
    "# **Logistic Regression vs Linear Regression via Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o2jeOJg80h6N"
   },
   "outputs": [],
   "source": [
    "# Let us first import some modules for this problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QRJDVn2lF7Pv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 1.24794646e-47 5.00000000e-01 5.79324252e-01]\n",
      "[0.67212099 0.5481529  0.5871972  0.62176131]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "  sigmoid function that maps inputs into the interval [0,1]\n",
    "  Your implementation must be able to handle the case when z is a vector (see unit test)\n",
    "  Inputs:\n",
    "  - z: a scalar (real number) or a vector\n",
    "  Outputs:\n",
    "  - trans_z: the same shape as z, with sigmoid applied to each element of z\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    if (isinstance(z, (list, np.ndarray))):\n",
    "        size = z.size\n",
    "        ans = np.zeros(size)\n",
    "        \n",
    "        for i in range(size):\n",
    "            ans[i] = 1/(1 + (math.exp(-z[i])))\n",
    "        \n",
    "        return ans\n",
    "    \n",
    "    trans_z = 1/(1 + (math.exp(-z)))\n",
    "            \n",
    "        \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return trans_z\n",
    "\n",
    "def logistic_regression(X, w):\n",
    "    \"\"\"\n",
    "  logistic regression model that outputs probabilities of positive examples\n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_features)\n",
    "  - w: an array of shape (num_features,)\n",
    "  Outputs:\n",
    "  - logits: a vector of shape (num_samples,)\n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    size = X.size\n",
    "    size2 = X[0].size\n",
    "    \n",
    "    logits = sigmoid(np.matmul(X,w))\n",
    "\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return logits\n",
    "\n",
    "# unit test\n",
    "# sample inputs:\n",
    "z = np.array([215, -108, 0, 0.32])\n",
    "X = np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
    "              [3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
    "              [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
    "              [5.38816734e-01, 4.19194514e-01, 6.85219500e-01]])\n",
    "w = np.array([0.20445225, 0.87811744, 0.02738759])\n",
    "\n",
    "# sample outputs:\n",
    "out1 = sigmoid(z)\n",
    "out2 = logistic_regression(X, w)\n",
    "print(out1)\n",
    "print(out2)\n",
    "# out1 : [1.00000000e+00 1.24794646e-47 5.00000000e-01 5.79324252e-01]\n",
    "# out2 : [0.67212099 0.5481529  0.5871972  0.62176131]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cyj_fVzBuo03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626238298577102\n",
      "[-0.00483685 -0.09821878 -0.0080873 ]\n"
     ]
    }
   ],
   "source": [
    "def logistic_loss(X, w, y):\n",
    "    \"\"\"\n",
    "  a function that compute the logistic loss value for the given dataset (X, y) and parameter w;\n",
    "  It also returns the gradient of loss function w.r.t w\n",
    "  Here (X, y) can be a set of examples, not just one example.\n",
    "  Remember -- we always take the average (divide by size of dataset) while calculating loss, gradient etc. \n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_features)\n",
    "  - w: an array of shape (num_features,)\n",
    "  - y: an array of shape (num_sample,), it is the ground truth label of data X\n",
    "  Output:\n",
    "  - loss: a scalar which is the value of loss function for the given data and parameters\n",
    "  - grad: an array of shape (num_featues,), the gradient of loss \n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    z = logistic_regression(X, w)\n",
    "    m = y.size\n",
    "    loss = 0\n",
    "    grad = (np.matmul(np.transpose(X),z-y))/m\n",
    "    \n",
    "    for i in range(m):\n",
    "        if (y[i]):\n",
    "            loss -= math.log(z[i])\n",
    "        else:\n",
    "            loss -= math.log(1-z[i])\n",
    "            \n",
    "    z = np.matmul(X,w)\n",
    "    loss = loss/m\n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss, grad\n",
    "#unit test\n",
    "# inputs:\n",
    "X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
    "              [0.14038694, 0.19810149, 0.80074457],\n",
    "              [0.96826158, 0.31342418, 0.69232262],\n",
    "              [0.87638915, 0.89460666, 0.08504421]])\n",
    "w = np.array([0.03905478, 0.16983042, 0.8781425 ])\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "\n",
    "# sample outputs:\n",
    "loss, grad = logistic_loss(X, w, Y)\n",
    "print(loss)\n",
    "print(grad)\n",
    "# loss: 0.626238298577102\n",
    "# grad: [-0.00483685, -0.09821878, -0.0080873 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31784857214588724\n",
      "[-0.14180542 -0.33264742 -0.007825  ]\n"
     ]
    }
   ],
   "source": [
    "def square_loss(X, w, y):\n",
    "    \"\"\"\n",
    "  a function that compute the linear regression loss value for the given dataset (X, y) and parameter w;\n",
    "  It also returns the gradient of loss function w.r.t w\n",
    "  Here (X, y) can be a set of examples, not just one example.\n",
    "  Remember -- we always take the average (divide by size of dataset) while calculating loss, gradient etc. \n",
    "  Inputs:\n",
    "  - X: an array of shape (num_sample, num_features)\n",
    "  - w: an array of shape (num_features,)\n",
    "  - y: an array of shape (num_sample,), it is the ground truth label of data X\n",
    "  Output:\n",
    "  - loss: a scalar which is the value of loss function for the given data and parameters\n",
    "  - grad: an array of shape (num_featues,), the gradient of loss \n",
    "  \"\"\"\n",
    "  # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    m = y.size\n",
    "    loss = (np.sum(np.power((np.matmul(X, w) - y),2)))/m\n",
    "    \n",
    "    grad = (2*(np.matmul(np.matmul(np.transpose(X),X),w)) - 2*(np.matmul(np.transpose(X),y)))/m\n",
    "        \n",
    "  # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss, grad\n",
    "#unit test\n",
    "# inputs:\n",
    "X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
    "              [0.14038694, 0.19810149, 0.80074457],\n",
    "              [0.96826158, 0.31342418, 0.69232262],\n",
    "              [0.87638915, 0.89460666, 0.08504421]])\n",
    "w = np.array([0.03905478, 0.16983042, 0.8781425 ])\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "\n",
    "# sample outputs:\n",
    "loss, grad = square_loss(X, w, Y)\n",
    "print(loss)\n",
    "print(grad)\n",
    "# loss: 0.317848572145887241\n",
    "# grad: [-0.14180542, -0.33264742, -0.007825 ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDSd4ABKUM3"
   },
   "source": [
    "## 1.2 Recognizing hand-written digits with logistic regression \n",
    "\n",
    "We have gone through all the theoretical concepts of the logistic regression model. It's time to put hands on a real problem in which we aim to recognize images of hand-written digits. The dataset we will use is the Optical Recognition of Handwritten Digits dataset, and the description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits). \n",
    "\n",
    "### 1.2.1 Data preprocessing (not for grading){-}\n",
    "The original dataset contains 10 classes (digits 0 to 9). Since for now we are concerned about logistic regression for binary classification, we will only use a subset of the dataset that contains 360 examples from 2 classes (digits 0 and 1).  Each example is a $8\\times 8$ matrix (image) where each element is an integer in the range $[0,16]$. Let's load the dataset by using the off-the-shell method from `sklearn` and print out some images to get a good understanding of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "4AX98obkFPLh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 360 examples in total.\n",
      "All examples are images of hand-written digit 0 or hand-written digits 1\n",
      "Each example is an array of shape (64,)\n",
      "An example of data point:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "The shape of image is (8, 8)\n",
      "An example of 2D array data:\n",
      " [[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# set up the code for this experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "# load the digits dataset\n",
    "digits = load_digits(n_class=2)\n",
    "# digits is a dictionary-like object that hold all the features and labels,\n",
    "# along with some metadata about the dataset. \n",
    "# The features are stored in the '.data' member, a (#sample, #feature) array. \n",
    "# The labels are stored in the '.target' member.\n",
    "\n",
    "print(f'There are {len(digits.target)} examples in total.')\n",
    "print(f'All examples are images of hand-written digit {list(set(digits.target))[0]} or hand-written digits {list(set(digits.target))[1]}')\n",
    "print(f'Each example is an array of shape {digits.data[0].shape}')\n",
    "print(f'An example of data point:\\n{digits.data[0]}')\n",
    "\n",
    "# You may wondering why the shape of data is (64,) instead of (8, 8). Actually,\n",
    "# You can access to matrix shape of data through the '.images' member.\n",
    "print(f'The shape of image is {digits.images[0].shape}') \n",
    "print(f'An example of 2D array data:\\n {digits.images[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kazXeCBCXLee"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The data we are interested in is made up of 8x8 images of digits. \n",
    "# Let's have a look at the first 6 images that are drawn from the dataset. \n",
    "# For these images, \n",
    "#   we know the digit they represented is given in the 'target' of the dataset.\n",
    "_, axes = plt.subplots(1, 6)\n",
    "img_label = list(zip(digits.images, digits.target))\n",
    "for ax, (img, target) in zip(axes, img_label[:6]):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "  ax.set_title('Label: %i' % target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oveqe9GK526A"
   },
   "source": [
    "### 1.2.2 Padding features (not for grading) {-}\n",
    "As we did in Homework 2, to simplify the notation, we pad the input $x$ by inserting 1 to the **beginning** so that we can absorb the bias term into the parameter $w$.\n",
    "\n",
    "The following code morphs the variable `digits.data` by concatenating 1 and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_h4w6JIZ8Cpk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 65)\n"
     ]
    }
   ],
   "source": [
    "ones = np.ones(digits.data.shape[0]).reshape(-1, 1)\n",
    "digits.data = np.concatenate((ones, digits.data), axis=1)\n",
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSwnaHBAoQbt"
   },
   "source": [
    "### 1.2.3 Create training and test sets (not for grading) {-}\n",
    "As we have practiced in our previous homework, we will use the `train_test_split()` method to partition the dataset into training and test sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, we will set the `random_state` argument of `train_test_split()` to **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Y89isTi9qLcV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 288 examples.\n",
      "The testing set contains 72 examples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.8, random_state=1)\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {X_test.shape[0]} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n39CSTpbvpHR"
   },
   "source": [
    "### 1.2.3 Feature Normalization (not for grading) {-}\n",
    "Since, we have implemented the function `featureNormalization()` to normalize the features that have different scale, now, we will learn to use the built-in function `StandardScaler()` in `scikit-learn`. As we did in `featureNormalization()`, `StandardScaler()` returns standardized features by removing the mean and scaling to unit variance.\n",
    "Please read through the [API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "nJV3UYO1z4Zk"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u69oMti4KhzJ"
   },
   "source": [
    "### Training the model with gradient descent \n",
    "Now after all the pre-processing, we can train a logisitic regression model with the training data.  It is quite straightforward to make predictions on test data by using the learned model. To simplify the task, when the probability of being positive is greater than 0.5, we classify the sample to 1. Otherwise, we classify it to 0.\n",
    "\n",
    "In this part, we will train the model with gradient descent. After that, predict the label for test examples and compute the test accuracy. You may want to follow the procedures below to obtain the results:\n",
    "+ Randomly initialize the parameter $w$ by `np.random.rand`.\n",
    "+ Use gradient descent to update $w$ (number of iteration `num_iters` and learning rate `lr` are provided).\n",
    "+ Plot the curve of the $\\ell(w)$ value as a function of how many update steps have been taken (you need a variable to store the history of $\\ell(w)$ values).\n",
    "+ Compute and report the test accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "yYGPnvLAEvkO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2707b9796c8>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_iters = 200\n",
    "lr = 0.1\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "w1 = np.random.rand(65)\n",
    "w2 = np.random.rand(65)\n",
    "n = X_train.size\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "\n",
    "# training\n",
    "for i in range(num_iters):\n",
    "    loss1, grad1 = square_loss(X_train, w1, y_train)\n",
    "    loss2, grad2 = logistic_loss(X_train, w2, y_train)\n",
    "        \n",
    "    w1 = w1 - (lr*grad1)\n",
    "    w2 = w2 - (lr*grad2)\n",
    "    \n",
    "    x.append(i)\n",
    "    y1.append(loss1)\n",
    "    y2.append(loss2)\n",
    "\n",
    "# plotting\n",
    "plt.plot(x, y2)\n",
    "\n",
    "# testing\n",
    "\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best accuracy\n",
    "lr: T = 800, lr = 0.6\n",
    "logr: T = 1000, lr = 0.8\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 , Q2 in the BB zip "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
