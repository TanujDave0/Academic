{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMM2e3OP6Kn4"
   },
   "source": [
    "# **Kernel SVM vs Logistic Regression via Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "o2jeOJg80h6N"
   },
   "outputs": [],
   "source": [
    "# Let us first import some modules for this problem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "QRJDVn2lF7Pv"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "#     \"\"\"\n",
    "#     sigmoid function that maps inputs into the interval [0,1]\n",
    "#     Your implementation must be able to handle the case when z is a vector (see unit test)\n",
    "#     Inputs:\n",
    "#     - z: a scalar (real number) or a vector\n",
    "#     Outputs:\n",
    "#     - trans_z: the same shape as z, with sigmoid applied to each element of z\n",
    "#     \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    trans_z = 1 / (1 + np.exp(-z))\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return trans_z\n",
    "\n",
    "def logistic_regression(X, w):\n",
    "#     \"\"\"\n",
    "#     logistic regression model that outputs probabilities of positive examples\n",
    "#     Inputs:\n",
    "#     - X: an array of shape (num_sample, num_features)\n",
    "#     - w: an array of shape (num_features,)\n",
    "#     Outputs:\n",
    "#     - logits: a vector of shape (num_samples,)\n",
    "#     \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    logits = sigmoid(np.dot(X,w))\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return logits\n",
    "\n",
    "# unit test\n",
    "# sample inputs:\n",
    "z = np.array([215, -108, 0, 0.32])\n",
    "X = np.array([[4.17022005e-01, 7.20324493e-01, 1.14374817e-04],\n",
    "              [3.02332573e-01, 1.46755891e-01, 9.23385948e-02],\n",
    "              [1.86260211e-01, 3.45560727e-01, 3.96767474e-01],\n",
    "              [5.38816734e-01, 4.19194514e-01, 6.85219500e-01]])\n",
    "w = np.array([0.20445225, 0.87811744, 0.02738759])\n",
    "\n",
    "# sample outputs:\n",
    "# out1 = sigmoid(z)\n",
    "# out1 : [1.00000000e+00 1.24794646e-47 5.00000000e-01 5.79324252e-01]\n",
    "# out2 = logistic_regression(X, w)\n",
    "# out2 : [0.67212099 0.5481529  0.5871972  0.62176131]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 1.24794646e-47 5.00000000e-01 5.79324252e-01] [0.67212099 0.5481529  0.5871972  0.62176131]\n"
     ]
    }
   ],
   "source": [
    "out1 = sigmoid(z)\n",
    "out2 = logistic_regression(X, w)\n",
    "print(out1,out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.  2.]\n",
      " [ 2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "def linear_kernel(X):\n",
    "#     \"\"\"\n",
    "#     a function that compute the Kernel Matrix K given X\n",
    "#     the entry at the i-th row, j-th column is given by the dot product\n",
    "#     between i, j  rows of data matrix X\n",
    "#     Inputs:\n",
    "#     - X: an array of shape (num_sample, num_features)\n",
    "#     Output:\n",
    "#     - K: an array of shape (num_sample, num_sample)\n",
    "#     \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    n = X.shape[0]\n",
    "    linK = np.zeros(shape=(n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            linK[i, j] = np.dot(X[i], X[j])\n",
    "            \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    return linK\n",
    "\n",
    "X = np.array([[1,3,1],\n",
    "              [1,0,1]])\n",
    "print(linear_kernel(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36787944]\n",
      " [0.36787944 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def gaussian_kernel(X, gamma = 1.0):\n",
    "#     \"\"\"\n",
    "#     a function that compute the Kernel Matrix K given X\n",
    "#     the entry at the i-th row, j-th column is given in the homework\n",
    "#     Inputs:\n",
    "#     - X: an array of shape (num_sample, num_features)\n",
    "#     Output:\n",
    "#     - K: an array of shape (num_sample, num_sample)\n",
    "#     \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    n = X.shape[0]\n",
    "    gaussK = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            gaussK[i, j] = np.exp(-1*gamma*(np.linalg.norm((X[i]-X[j]), ord=2)**2))\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    return gaussK\n",
    "\n",
    "X = np.array([[1,1,1],\n",
    "              [1,0,1]])\n",
    "print(gaussian_kernel(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Please check notes for derivation of gradient of hinge loss**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9988814761367293 [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def hinge_loss(K, w, y, lamda = 1.0):\n",
    "#     \"\"\"\n",
    "#     a function that compute the SVM Hinge loss value for the given dataset (X, y) and parameter w;\n",
    "#     It also returns the gradient of loss function w.r.t w\n",
    "#     Here (X, y) can be a set of examples, not just one example.\n",
    "#     Remember -- we always take the average (divide by size of dataset) while calculating loss, gradient etc. \n",
    "#     Inputs:\n",
    "#     - X: an array of shape (num_sample, num_features)\n",
    "#     - w: an array of shape (num_features,)\n",
    "#     - y: an array of shape (num_sample,), it is the ground truth label of data X\n",
    "#     Output:\n",
    "#     - loss: a scalar which is the value of loss function for the given data and parameters\n",
    "#     - grad: an array of shape (num_featues,), the gradient of loss \n",
    "#     \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    nf = K.shape[1]\n",
    "    n = K.shape[0]\n",
    "    grad = np.zeros(nf)\n",
    "    \n",
    "    temp = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if 1-y[i]*np.matmul(w, K.T)[i] > 0:\n",
    "            temp[i] = 1\n",
    "        else:\n",
    "            temp[i] = 0\n",
    "    \n",
    "    loss = (np.matmul(np.matmul(w, K.T), w)/2) + (lamda/n)*(np.sum(temp))\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss, grad\n",
    "#unit test -- write your own unit by following other unit tests\n",
    "X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
    "              [0.14038694, 0.19810149, 0.80074457],\n",
    "              [0.96826158, 0.31342418, 0.69232262],\n",
    "              [0.87638915, 0.89460666, 0.08504421]])\n",
    "w = np.array([0.03905478, 0.16983042, 0.8781425, 1 ])\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "\n",
    "loss,grad = hinge_loss(gaussian_kernel(X), w, Y)\n",
    "print(loss,grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "cyj_fVzBuo03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626238298577102 [-0.00483685 -0.09821878 -0.0080873 ]\n"
     ]
    }
   ],
   "source": [
    "def logistic_loss(K, w, y, lamda = 1.0):\n",
    "#     \"\"\"\n",
    "#     a function that compute the logistic loss value for the given dataset represented by the Kernel Matrix K and parameter w;\n",
    "#     It also returns the gradient of loss function w.r.t w, \n",
    "#     Please watch Lecture 23, this is not the same as Homework 3\n",
    "#     Remember -- we always take the average (divide by size of dataset) while calculating loss, gradient etc. \n",
    "#     Inputs:\n",
    "#     - X: an array of shape (num_sample, num_features)\n",
    "#     - w: an array of shape (num_features,)\n",
    "#     - y: an array of shape (num_sample,), it is the ground truth label of data X\n",
    "#     Output:\n",
    "#     - loss: a scalar which is the value of loss function for the given data and parameters\n",
    "#     - grad: an array of shape (num_featues,), the gradient of loss \n",
    "#     \"\"\"\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    #assume gaussian kernel or oth\n",
    "\n",
    "    a = logistic_regression(K,w)\n",
    "    \n",
    "    num_train = K.shape[0]\n",
    "    \n",
    "    \n",
    "    loss = -1/num_train * np.sum(y*np.log(a) + (1-y)*np.log(1-a))\n",
    "    \n",
    "    grad = np.dot(K.T, (a-y))/num_train\n",
    "    \n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss, grad\n",
    "#unit test\n",
    "# You may use the unit given in Homework 3 (with linear kernel)\n",
    "X = np.array([[0.67046751, 0.41730480, 0.55868983],\n",
    "              [0.14038694, 0.19810149, 0.80074457],\n",
    "              [0.96826158, 0.31342418, 0.69232262],\n",
    "              [0.87638915, 0.89460666, 0.08504421]])\n",
    "w = np.array([0.03905478, 0.16983042, 0.8781425 ])\n",
    "Y = np.array([1, 1, 0, 1])\n",
    "\n",
    "loss,grad = logistic_loss(X, w, Y)\n",
    "print(loss,grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss,grad = hinge_loss(K, w, y)\n",
    "#loss, grad = logistic_loss(K, w, y)\n",
    "#print(loss,grad) \n",
    "#grad should be of the same size of the size of w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "4AX98obkFPLh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the code for this experiment\n",
    "# set up code for this experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "url = 'https://www.dropbox.com/s/tacy227l893y9r1/spambase.data?dl=1'\n",
    "file_name = 'spambase.data'\n",
    "with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "\n",
    "df = pd.read_csv(file_name,\n",
    "                    sep=',',\n",
    "                    header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data matrix X from df.head()\n",
    "hatX = df.to_numpy()\n",
    "y = hatX[:, -1] # for last column\n",
    "X = hatX[:, :-1] # for all but last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n"
     ]
    }
   ],
   "source": [
    "print(hatX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oveqe9GK526A"
   },
   "source": [
    "### 1.2.2 Padding features (not for grading) {-}\n",
    "As we did in Homework 2, to simplify the notation, we pad the input $x$ by inserting 1 to the **beginning** so that we can absorb the bias term into the parameter $w$.\n",
    "\n",
    "The following code morphs the variable `digits.data` by concatenating 1 and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "_h4w6JIZ8Cpk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n"
     ]
    }
   ],
   "source": [
    "ones = np.ones(X.shape[0]).reshape(-1, 1)\n",
    "X = np.concatenate((ones, X), axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSwnaHBAoQbt"
   },
   "source": [
    "### 1.2.3 Create training and test sets (not for grading) {-}\n",
    "As we have practiced in our previous homework, we will use the `train_test_split()` method to partition the dataset into training and test sets. In this experiment, we use 80% data for training and the remaining 20% data for testing. To ensure your results are replicable, we will set the `random_state` argument of `train_test_split()` to **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "Y89isTi9qLcV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 3680 examples.\n",
      "The testing set contains 921 examples.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)\n",
    "print(f'The training set contains {X_train.shape[0]} examples.')\n",
    "print(f'The testing set contains {X_test.shape[0]} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n39CSTpbvpHR"
   },
   "source": [
    "### 1.2.3 Feature Normalization (not for grading) {-}\n",
    "Since, we have implemented the function `featureNormalization()` to normalize the features that have different scale, now, we will learn to use the built-in function `StandardScaler()` in `scikit-learn`. As we did in `featureNormalization()`, `StandardScaler()` returns standardized features by removing the mean and scaling to unit variance.\n",
    "Please read through the [API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "nJV3UYO1z4Zk"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u69oMti4KhzJ"
   },
   "source": [
    "### Training the model with gradient descent \n",
    "Now after all the pre-processing, we can train a logisitic regression model with the training data.  It is quite straightforward to make predictions on test data by using the learned model. To simplify the task, when the probability of being positive is greater than 0.5, we classify the sample to 1. Otherwise, we classify it to 0.\n",
    "\n",
    "In this part, we will train the model with gradient descent. After that, predict the label for test examples and compute the test accuracy. You may want to follow the procedures below to obtain the results:\n",
    "+ Randomly initialize the parameter $w$ by `np.random.rand`.\n",
    "+ Use gradient descent to update $w$ (number of iteration `num_iters` and learning rate `lr` are provided).\n",
    "+ Plot the curve of the $\\ell(w)$ value as a function of how many update steps have been taken (you need a variable to store the history of $\\ell(w)$ values).\n",
    "+ Compute and report the test accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "yYGPnvLAEvkO"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-325955e469a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhinge_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-02938670a425>\u001b[0m in \u001b[0;36mhinge_loss\u001b[1;34m(K, w, y, lamda)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iters = 1000\n",
    "lr = 0.1\n",
    "count = 0.0\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "w = np.random.rand(X_train.shape[1])\n",
    "X_train = gaussian_kernel(X_train)\n",
    "loss_list = []\n",
    "for i in range(num_iters):\n",
    "    loss, grad = hinge_loss(X_train, w, y_train)\n",
    "    loss_list.append(loss)\n",
    "    w = w - lr*grad\n",
    "# plotting\n",
    "plt.plot(np.arange(num_iters), loss_list)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "# testing\n",
    "# pred = sigmoid(np.dot(X_test,w))\n",
    "# for i in range(len(pred)):\n",
    "#     if pred[i] > 0.5:\n",
    "#         pred[i] = 1\n",
    "#     else:\n",
    "#         pred[i] = 0\n",
    "\n",
    "# count = 0\n",
    "# for i in range(len(pred)):\n",
    "#     if pred[i] == y_test[i]:\n",
    "#         count+=1\n",
    "# print(\"test accuracy: \", count/len(pred))\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-95f23e338191>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhinge_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-02938670a425>\u001b[0m in \u001b[0;36mhinge_loss\u001b[1;34m(K, w, y, lamda)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iters = [100,200,500,1000]\n",
    "lr = [0.01, 0.1, 0.2, 0.5]\n",
    "\n",
    "w = np.random.rand(X_train.shape[1])\n",
    "# X_train = gaussian_kernel(X_train)\n",
    "loss_list = []\n",
    "for i in range(num_iters[1]):\n",
    "    loss, grad = hinge_loss(X_train, w, y_train)\n",
    "    loss_list.append(loss)\n",
    "    w = w - lr[1]*grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the training size increases the effectiveness of the SVM and log-loss, same for the lr. Overall, the gaussian kernel is much more effective than the linear kernel.\n",
    "\n",
    "The accuracies increase with number of iternations and learning rate. 1000 iterations and 0.2 lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1 and Q2 on BB notes**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
