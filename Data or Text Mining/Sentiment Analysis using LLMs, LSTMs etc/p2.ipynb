{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "executionInfo": {
     "elapsed": 20836,
     "status": "ok",
     "timestamp": 1713557441391,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "D0Butuj4GX9I",
    "outputId": "9c75eaeb-3988-4732-b151-3cd7457b93c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libs loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, MaxPooling1D, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, AveragePooling1D, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spacy\n",
    "\n",
    "print('libs loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'time', 'Anootated tweet', 'Class'], dtype='object')\n",
      "Index(['date', 'time', 'Anootated tweet', 'Class'], dtype='object')\n",
      "db shape: (7200, 4)\n",
      "\n",
      "db shape: (5648, 2)\n",
      "\n",
      "tweet    object\n",
      "class     int32\n",
      "dtype: object\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hope &lt;e&gt;Romney&lt;/e&gt; debate prepped w/ the same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  class\n",
       "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...     -1\n",
       "1  .@WardBrenda @shortwave8669 @allanbourdius you...     -1\n",
       "2  <e>Mitt Romney</e> still doesn't <a>believe</a...     -1\n",
       "3  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...     -1\n",
       "4  Hope <e>Romney</e> debate prepped w/ the same ...      1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_EMBEDDING_SIZE = 200\n",
    "TWEET_MAX_LENGTH = 150\n",
    "\n",
    "glove_file_path = 'glove.twitter.27B.200d.txt'\n",
    "\n",
    "db1_path = 'obama.csv'\n",
    "db2_path = 'romney.csv'\n",
    "\n",
    "# db1 = pd.read_csv(db1_path, encoding='macroman')\n",
    "db2 = pd.read_csv(db2_path, encoding='macroman')\n",
    "\n",
    "print(db1.columns)\n",
    "print(db2.columns)\n",
    "\n",
    "# db = pd.concat([db1, db2], axis=0)\n",
    "# db = db1\n",
    "db = pd.read_csv(db2_path, encoding='macroman')\n",
    "\n",
    "print('db shape:', db.shape)\n",
    "print()\n",
    "\n",
    "db.drop(columns = ['date', 'time'], inplace = True)\n",
    "db['Class'] = pd.to_numeric(db['Class'], errors='coerce')\n",
    "db = db.dropna()\n",
    "\n",
    "db['Class'] = db['Class'].astype(int)\n",
    "db.rename(columns = {'Anootated tweet':'tweet', 'Class': 'class'}, inplace = True)\n",
    "\n",
    "#dropping class 2\n",
    "db = db[db['class'] != 2]\n",
    "\n",
    "db.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('db shape:', db.shape)\n",
    "print()\n",
    "print(db.dtypes)\n",
    "print(db.isnull().values.any())\n",
    "\n",
    "db.head()\n",
    "# testdb = pd.read_csv('/content/drive/MyDrive/CS 583 P2/sample-testdata.csv', header = None)\n",
    "\n",
    "# print(testdb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 674,
     "status": "ok",
     "timestamp": 1713557442043,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "bMfHHOPrkP_D",
    "outputId": "06283801-61c0-4fe2-96cd-eb2d65464b90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='class', ylabel='count'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='class', data=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1713557442445,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "F_marQKDu25c",
    "outputId": "a07b62c2-ed26-4626-af86-be591ed2731c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tanuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "  return word_tokenize(tweet)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    tweet = re.sub('<.*?>', '', tweet)\n",
    "\n",
    "    # Remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    processed_tweet = ' '.join(tokens)\n",
    "\n",
    "    return processed_tweet\n",
    "\n",
    "def new_preprocess_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "        \n",
    "    # Remove HTML tags\n",
    "    tweet = re.sub('<.*?>', '', tweet)\n",
    "        \n",
    "    tokens = tweet.split()\n",
    "    \n",
    "     # Remove URLs, user mentions, numbers, and special characters\n",
    "    tokens = [token for token in tokens if not token.startswith('@') and not token.startswith('http')]\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # Remove tokens with special characters except punctuation\n",
    "    pattern = r'[^a-zA-Z0-9\\s{}]'.format(re.escape(string.punctuation))\n",
    "    \n",
    "    tokens = [re.sub(pattern, '', token) for token in tokens]\n",
    "    \n",
    "    tweet = ' '.join(tokens)\n",
    "        \n",
    "    # Tokenize tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "        \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    # Replace multiple same characters at the end of a word with only two of those characters\n",
    "    tokens = [re.sub(r'(.)\\1+$', r'\\1\\1', token) for token in tokens]\n",
    "        \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "    # Lemmatize tokens\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # # temp = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "    # Join tokens back into a string\n",
    "    processed_tweet = ' '.join(tokens)\n",
    "\n",
    "    return processed_tweet\n",
    "\n",
    "def new_preprocess_tweet_v2(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "        \n",
    "    # Remove HTML tags\n",
    "    tweet = re.sub('<.*?>', '', tweet)\n",
    "        \n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    \n",
    "     # Remove URLs, user mentions, numbers, and special characters\n",
    "    tokens = [token for token in tokens if not token.startswith('@') and not token.startswith('http')]\n",
    "    tokens = [token for token in tokens if not token.isdigit()]\n",
    "    \n",
    "    # Remove tokens with special characters except punctuation\n",
    "    pattern = r'[^a-zA-Z0-9\\s{}]'.format(re.escape(string.punctuation))\n",
    "    \n",
    "    tokens = [re.sub(pattern, '', token) for token in tokens]\n",
    "    \n",
    "    tweet = ' '.join(tokens)\n",
    "        \n",
    "    # Tokenize tweet\n",
    "    # tokens = word_tokenize(tweet)\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    doc = nlp(tweet)\n",
    "    tokens = [token.text for token in doc]\n",
    "        \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "        \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "    # Lemmatize tokens\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # # temp = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "    # Join tokens back into a string\n",
    "    processed_tweet = ' '.join(tokens)\n",
    "\n",
    "    return processed_tweet\n",
    "\n",
    "def preprocess_tweets(df):\n",
    "    df['tweet'] = df['tweet'].apply(new_preprocess_tweet)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9476,
     "status": "ok",
     "timestamp": 1713557451907,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "28Rv9VRQ5HrK",
    "outputId": "30f841f2-87c3-46a0-8139-01864351acc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Hope the <a>debate</a> goes as well for <e>Romney </e>tonight as it did the first time around. But, with hair like that, how could it not?? @Mollaysia\n",
      "before: <e>Romney</e> will \"harvest\" all of us. (@YouTube http://t.co/NqG7JM69)\n",
      "before: If women that the Polls show are truly voting for <e>Romney</e>? Then I don't want to know how they truly take care of their household's.\n",
      "\n",
      "after: hope debate goes well romney tonight first time around hair like could\n",
      "after: romney `` harvest '' us youtube\n",
      "after: women polls show truly voting romney n't want know truly take care household 's\n"
     ]
    }
   ],
   "source": [
    "print('before:', db['tweet'][20])\n",
    "print('before:', db['tweet'][999])\n",
    "print('before:', db['tweet'][11])\n",
    "print()\n",
    "\n",
    "db = preprocess_tweets(db)\n",
    "\n",
    "#dropping class 2\n",
    "db = db[db['class'] != 2]\n",
    "\n",
    "db['class'] = db['class'].replace({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "print('after:', db['tweet'][20])\n",
    "print('after:', db['tweet'][999])\n",
    "print('after:', db['tweet'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yayyyy\n",
      "yayyyy\n",
      "yayy\n",
      "yayyyy\n"
     ]
    }
   ],
   "source": [
    "tweet = 'yayyyy'\n",
    "\n",
    "print(tweet)\n",
    "print(preprocess_tweet(tweet))\n",
    "\n",
    "print(new_preprocess_tweet(tweet))\n",
    "\n",
    "print(new_preprocess_tweet_v2(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debate', '#', '2', 'tonight', '!', 'I', 'hope', '<e>', 'Obama', '</e>', 'brings', 'a', 'lot', 'of', 'energy', 'tonight', '.', 'This', 'debate', 'is', 'crucial', '#obama2012']\n",
      "['debate', '#', '2', 'tonight', '!', 'I', 'hope', '<', 'e', '>', 'Obama', '<', '/e', '>', 'brings', 'a', 'lot', 'of', 'energy', 'tonight', '.', 'This', 'debate', 'is', 'crucial', '#', 'obama2012']\n",
      "['debate', '#', '2', 'tonight', '!', 'I', 'hope', '<', 'e', '>', 'Obama</e', '>', 'brings', 'a', 'lot', 'of', 'energy', 'tonight', '.', 'This', 'debate', 'is', 'crucial', '#', 'obama2012']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "# text = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "text = db['tweet'][20]\n",
    "tokens = tknzr.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkqYHcnJ7dbI"
   },
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 77000,
     "status": "ok",
     "timestamp": 1713557528874,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "reMVUJFmuHha"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open(glove_file_path, encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.array(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "print('embeddings loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 8136,
     "status": "ok",
     "timestamp": 1713557537003,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "QlNsJ4hU7nNx"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "temp = set()\n",
    "temp2 = set()\n",
    "\n",
    "def create_embedding_matrix(embeddings_dictionary, tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_SIZE))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in embeddings_dictionary:\n",
    "            embedding_matrix[i] = embeddings_dictionary[word]\n",
    "        else:\n",
    "            temp.add(word)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def create_embedding_matrix_using_map(embeddings_dictionary, unique_tokens):\n",
    "    vocab_size = len(unique_tokens)\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, WORD_EMBEDDING_SIZE))\n",
    "\n",
    "    for word, i in unique_tokens.items():\n",
    "        if word in embeddings_dictionary:\n",
    "            embedding_matrix[i] = embeddings_dictionary[word]\n",
    "        else:\n",
    "            temp2.add(word)\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_token_map(tweets):\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tokens = word_tokenize(tweet.lower())\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    token_map = {token: index for index, token in enumerate(unique_tokens)}\n",
    "\n",
    "    return token_map\n",
    "\n",
    "\n",
    "word_tokenizer_keras = Tokenizer()\n",
    "word_tokenizer_keras.fit_on_texts(db['tweet'])\n",
    "\n",
    "nltk_token_map = get_token_map(db['tweet'])\n",
    "\n",
    "# Create the embedding matrix\n",
    "glovee_embedding_matrix = create_embedding_matrix(embeddings_dictionary, word_tokenizer_keras)\n",
    "glovee_embedding_matrix_2 = create_embedding_matrix_using_map(embeddings_dictionary, nltk_token_map)\n",
    "\n",
    "sentences = [word_tokenize(tweet) for tweet in db['tweet']]\n",
    "w2v_model = Word2Vec(sentences=sentences, vector_size = WORD_EMBEDDING_SIZE,\n",
    "                     window=10,\n",
    "                     min_count=1,\n",
    "                     workers=4)\n",
    "\n",
    "# Create the embedding matrix for Word2Vec\n",
    "w2v_embedding_matrix = create_embedding_matrix(w2v_model.wv, word_tokenizer_keras)\n",
    "w2v_embedding_matrix_2 = create_embedding_matrix_using_map(w2v_model.wv, nltk_token_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dne\n"
     ]
    }
   ],
   "source": [
    "def get_token_mapftw(tweets):\n",
    "    tknzr = TweetTokenizer()\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tokens = tknzr.tokenize(tweet.lower())\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    token_map = {token: index for index, token in enumerate(unique_tokens)}\n",
    "\n",
    "    return token_map\n",
    "\n",
    "def get_token_map_spacy(nlp, tweets):\n",
    "    unique_tokens = set()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tokens = [token.text for token in nlp(tweet.lower())]\n",
    "        unique_tokens.update(tokens)\n",
    "\n",
    "    token_map = {token: index for index, token in enumerate(unique_tokens)}\n",
    "\n",
    "    return token_map\n",
    "\n",
    "# Load the Spacy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Get unique tokens using Spacy for tokenization\n",
    "spacy_token_map = get_token_map_spacy(nlp, db['tweet'])\n",
    "\n",
    "# Create the embedding matrix\n",
    "# Get unique tokens\n",
    "nltkftw_token_map = get_token_mapftw(db['tweet'])\n",
    "\n",
    "# Create the embedding matrix\n",
    "glovee_embedding_matrix_spacy = create_embedding_matrix_using_map(embeddings_dictionary, spacy_token_map)\n",
    "glovee_embedding_matrix_nltkftw = create_embedding_matrix_using_map(embeddings_dictionary, nltkftw_token_map)\n",
    "\n",
    "print('dne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1713557537005,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "tNHZWPf6wY8Q",
    "outputId": "807103ee-34e0-4a61-c7f8-500fa1570387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2623 3457\n"
     ]
    }
   ],
   "source": [
    "# a little analysis on difference in words with no embedding in preloaded vs trained embedders and tokenizers\n",
    "print(len(temp), len(temp2))\n",
    "\n",
    "temp = list(temp)\n",
    "temp2 = list(temp2)\n",
    "\n",
    "# print(nltk_token_map)\n",
    "\n",
    "# for i in range(100):\n",
    "#   print(i, temp[i], temp2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intellectual'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_preprocess_tweet('intellectual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary[\":->\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 15718,
     "status": "ok",
     "timestamp": 1713557552709,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "7d1XBGpZ_oJW"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_tweets(tokenizer, text_data):\n",
    "    return tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "def get_nltk_tokenized(nltk_token_map, tweets):\n",
    "    tokens = [tokenize_tweet(tw) for tw in tweets]\n",
    "    tokens = [[nltk_token_map[w] for w in tw] for tw in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tweet_to_emb(tokenized_tweet, embedding_matrix):\n",
    "    embeddings = []\n",
    "\n",
    "    for token in tokenized_tweet:\n",
    "        # if token in model: # ignore the unknown for now\n",
    "      embeddings.append(embedding_matrix[token])\n",
    "\n",
    "    # padded_embeddings = pad_sequences([embeddings], maxlen = TWEET_MAX_LENGTH, dtype = 'float32', padding = 'post')[0]\n",
    "\n",
    "    # print(len(padded_embeddings), len(padded_embeddings[0]))\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def get_emb_tweets(tokenized_tweets, embedding_matrix):\n",
    "    ret = [tweet_to_emb(tw, embedding_matrix) for tw in tokenized_tweets]\n",
    "    # print(ret)\n",
    "    return np.array(ret)\n",
    "\n",
    "# embedded_tweets = get_emb_tweets(glovee_embedding_matrix, word_tokenizer_keras)\n",
    "\n",
    "# print(len(db['tweet']))\n",
    "# print(embedded_tweets.shape)\n",
    "\n",
    "\n",
    "#-------------tokenizing--------------------\n",
    "tokenized_tweets_keras = get_tokenized_tweets(word_tokenizer_keras, db['tweet'])\n",
    "\n",
    "tokenized_tweets_nltk = get_nltk_tokenized(nltk_token_map, db['tweet'])\n",
    "\n",
    "\n",
    "tokenized_tweets_keras = pad_sequences(tokenized_tweets_keras, maxlen = TWEET_MAX_LENGTH, padding = 'post')\n",
    "\n",
    "tokenized_tweets_nltk = pad_sequences(tokenized_tweets_nltk, maxlen = TWEET_MAX_LENGTH, padding = 'post')\n",
    "\n",
    "# print(tokenized_tweets_keras[0][0])\n",
    "\n",
    "#-------------embedding----------------------\n",
    "embedded_tweets_nltk_glovee = get_emb_tweets(tokenized_tweets_nltk, glovee_embedding_matrix_2)\n",
    "embedded_tweets_nltk_w2v = get_emb_tweets(tokenized_tweets_nltk, w2v_embedding_matrix_2)\n",
    "\n",
    "embedded_tweets_keras_glovee = get_emb_tweets(tokenized_tweets_keras, glovee_embedding_matrix)\n",
    "embedded_tweets_keras_w2v = get_emb_tweets(tokenized_tweets_keras, w2v_embedding_matrix)\n",
    "\n",
    "#-------------padding-------------------------\n",
    "tokenized_padded_tweets = pad_sequences(tokenized_tweets_nltk, maxlen = TWEET_MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dne\n"
     ]
    }
   ],
   "source": [
    "def get_nltk_tokenizedftw(nltkftw_token_map, tweets):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = [tknzr.tokenize(tw) for tw in tweets]\n",
    "    tokens = [[nltkftw_token_map[w] for w in tw] for tw in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_spacy_tokenized(nlp, token_map, tweets):\n",
    "    tokens = [[token.text for token in nlp(tw)] for tw in tweets]\n",
    "    tokens = [[token_map.get(w, 0) for w in tw] for tw in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokenized_tweets_spacy = get_spacy_tokenized(nlp, spacy_token_map, db['tweet'])\n",
    "tokenized_tweets_nltkftw = get_nltk_tokenizedftw(nltkftw_token_map, db['tweet'])\n",
    "\n",
    "tokenized_tweets_spacy = pad_sequences(tokenized_tweets_spacy, maxlen = TWEET_MAX_LENGTH, padding = 'post')\n",
    "tokenized_tweets_nltkftw = pad_sequences(tokenized_tweets_nltkftw, maxlen = TWEET_MAX_LENGTH, padding = 'post')\n",
    "\n",
    "embedded_tweets_spacy_glovee = get_emb_tweets(tokenized_tweets_spacy, glovee_embedding_matrix_spacy)\n",
    "embedded_tweets_nltkftw_glovee = get_emb_tweets(tokenized_tweets_nltkftw, glovee_embedding_matrix_nltkftw)\n",
    "print('dne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 851,
     "status": "ok",
     "timestamp": 1713557553548,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "ukgLGAyEIpPh"
   },
   "outputs": [],
   "source": [
    "#-------------Train Test Split-----------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedded_tweets_keras_glovee, np.array(db['class']), test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1713557553550,
     "user": {
      "displayName": "Tanuj Dave",
      "userId": "14568877915721362806"
     },
     "user_tz": 300
    },
    "id": "EfyyO6HfCpBt",
    "outputId": "b7b54245-978e-4bf2-dcfe-ad4a5ee4d444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: 4780   150\n",
      "5624   150   200\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", len(X_train), ' ', len(X_train[0]))\n",
    "print(len(embedded_tweets_nltk_glovee), ' ', len(embedded_tweets_nltk_glovee[0]), ' ', len(embedded_tweets_nltk_glovee[0][0]))\n",
    "# print(X_train[0])\n",
    "# print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlX_fh4mEfmK"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    print(f\"Test Loss: {loss}\")\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Predict the labels for the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "z_kvp8CLEeaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.5287 - loss: 0.9974 - val_accuracy: 0.5625 - val_loss: 0.9249\n",
      "Epoch 2/3\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.6577 - loss: 0.7872 - val_accuracy: 0.5750 - val_loss: 0.9246\n",
      "Epoch 3/3\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.7511 - loss: 0.6197 - val_accuracy: 0.5375 - val_loss: 1.0092\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5524 - loss: 1.0250\n",
      "Test Loss: 0.9853312373161316\n",
      "Test Accuracy: 0.5683962106704712\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "F1 Score: 0.5464\n",
      "Precision: 0.5480\n",
      "Recall: 0.5684\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = len(word_tokenizer_keras.word_index) + 1\n",
    "# vocab_size = len(nltk_token_map)\n",
    "\n",
    "# X = pad_sequences(tokenized_tweets_nltk, maxlen = TWEET_MAX_LENGTH, padding='post')\n",
    "\n",
    "# print(X.shape)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, np.array(db['class']), test_size = 0.15)\n",
    "clear_session()\n",
    "\n",
    "model_ff_nn = Sequential([\n",
    "    # Embedding(vocab_size, WORD_EMBEDDING_SIZE),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),  # Dense layer with 64 units and ReLU activation\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # Output layer with 3 units for classification\n",
    "])\n",
    "\n",
    "\n",
    "#--------------compile model-----------------\n",
    "model_ff_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#-----------------Train-----------------------\n",
    "model_ff_nn.fit(X_train, y_train, epochs = 3, validation_split = 0.05)\n",
    "\n",
    "#-----------------Test-----------------------\n",
    "evaluate_model(model_ff_nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "0MHKUOfXP5yu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.5055 - loss: 1.0178 - val_accuracy: 0.5792 - val_loss: 0.9279\n",
      "Epoch 2/5\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.6014 - loss: 0.8895 - val_accuracy: 0.5250 - val_loss: 0.9844\n",
      "Epoch 3/5\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.6517 - loss: 0.7884 - val_accuracy: 0.6042 - val_loss: 1.0085\n",
      "Epoch 4/5\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.7298 - loss: 0.6318 - val_accuracy: 0.5917 - val_loss: 1.0567\n",
      "Epoch 5/5\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8135 - loss: 0.4489 - val_accuracy: 0.5333 - val_loss: 1.3291\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5001 - loss: 1.3411\n",
      "Test Loss: 1.3193691968917847\n",
      "Test Accuracy: 0.5117924809455872\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "F1 Score: 0.5214\n",
      "Precision: 0.5514\n",
      "Recall: 0.5118\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate\n",
    "learning_rate = 0.002  # Example learning rate, you can adjust this value\n",
    "\n",
    "# Compile the model with the Adam optimizer and the specified learning rate\n",
    "# model_conv_nn = Sequential([\n",
    "#     Conv1D(100, 3, activation='relu'),\n",
    "#     AveragePooling1D(2),\n",
    "#     Conv1D(100, 4, activation='relu'),\n",
    "#     AveragePooling1D(2),\n",
    "#     Conv1D(100, 5, activation='relu'),\n",
    "#     Flatten(),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model_conv_nn = Sequential([\n",
    "    # Embedding(vocab_size, 150),\n",
    "    Conv1D(100, 5, activation='relu'),\n",
    "    AveragePooling1D(2),\n",
    "    Conv1D(100, 6, activation='relu'),\n",
    "    # AveragePooling1D(2),\n",
    "    # Conv1D(200, 7, activation='relu'),\n",
    "    AveragePooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_conv_nn.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_conv_nn.fit(X_train, y_train, epochs = 5, batch_size = 20, validation_split = 0.05)\n",
    "\n",
    "#-----------------Test-----------------------\n",
    "evaluate_model(model_conv_nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeLU07PPJbPb"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def train_and_plot(X_train, X_test, y_train, y_test, epochs=10):\n",
    "#     history = []\n",
    "\n",
    "#     model_conv_nn_1 = Sequential([\n",
    "#         Conv1D(100, 3, activation='relu'),\n",
    "#         AveragePooling1D(2),\n",
    "#         Conv1D(100, 4, activation='relu'),\n",
    "#         AveragePooling1D(2),\n",
    "#         Conv1D(100, 5, activation='relu'),\n",
    "#         Flatten(),\n",
    "#         Dense(128, activation='relu'),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(3, activation='softmax')\n",
    "#     ])\n",
    "\n",
    "#     model_ff_nn_1 = Sequential([\n",
    "#         Flatten(input_shape=(TWEET_MAX_LENGTH, WORD_EMBEDDING_SIZE)),  # Flatten the input\n",
    "#         Dense(64, activation='relu'),  # Dense layer with 64 units and ReLU activation\n",
    "#         Dropout(0.3),\n",
    "#         Dense(3, activation='softmax')  # Output layer with 3 units for classification\n",
    "#     ])\n",
    "\n",
    "#     models = [\n",
    "#         ['model_ff_nn_1', model_ff_nn_1],\n",
    "#         ['model_conv_nn_1', model_conv_nn_1]\n",
    "#     ]\n",
    "\n",
    "#     for [name, model] in models:\n",
    "#         model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#         print(f\"Training model: {name}\")\n",
    "\n",
    "#         hist = model.fit(X_train, y_train, epochs = epochs, validation_split = 0.1, verbose = 0)\n",
    "\n",
    "#         history.append(hist)\n",
    "\n",
    "#         loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "#         print(f\"Test Loss for {name}: {loss}\")\n",
    "#         print(f\"Test Accuracy for {name}: {accuracy}\")\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "\n",
    "#     for i, hist in enumerate(history):\n",
    "#         plt.plot(hist.history['val_accuracy'], label = models[i][0])\n",
    "\n",
    "#     plt.title('Validation Accuracy vs. Epochs')\n",
    "#     plt.xlabel('Epochs')\n",
    "#     plt.ylabel('Validation Accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "#     clear_session()\n",
    "\n",
    "\n",
    "# embeddings = {\n",
    "#     'embedded_tweets_nltk_glovee': embedded_tweets_nltk_glovee,\n",
    "#     'embedded_tweets_nltk_w2v': embedded_tweets_nltk_w2v,\n",
    "#     'embedded_tweets_keras_w2v': embedded_tweets_keras_w2v,\n",
    "#     'embedded_tweets_keras_glovee': embedded_tweets_keras_glovee\n",
    "# }\n",
    "\n",
    "# # Train and plot the models\n",
    "# for name, embedding in embeddings.items():\n",
    "#     print('for:', name)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(embedding, np.array(db['class']), test_size=0.15)\n",
    "\n",
    "#     train_and_plot(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "CHUs3SIrVnY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 489ms/step - accuracy: 0.5172 - loss: 1.0906 - val_accuracy: 0.5333 - val_loss: 1.0046\n",
      "Epoch 2/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 459ms/step - accuracy: 0.5147 - loss: 1.0175 - val_accuracy: 0.5333 - val_loss: 1.0051\n",
      "Epoch 3/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 478ms/step - accuracy: 0.5091 - loss: 1.0240 - val_accuracy: 0.5333 - val_loss: 1.0063\n",
      "Epoch 4/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 475ms/step - accuracy: 0.5126 - loss: 1.0193 - val_accuracy: 0.5333 - val_loss: 1.0075\n",
      "Epoch 5/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 480ms/step - accuracy: 0.5016 - loss: 1.0318 - val_accuracy: 0.5333 - val_loss: 1.0060\n",
      "Epoch 6/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 456ms/step - accuracy: 0.4944 - loss: 1.0257 - val_accuracy: 0.5333 - val_loss: 1.0047\n",
      "Epoch 7/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 463ms/step - accuracy: 0.5165 - loss: 1.0133 - val_accuracy: 0.5333 - val_loss: 1.0116\n",
      "Epoch 8/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 483ms/step - accuracy: 0.5278 - loss: 1.0109 - val_accuracy: 0.5333 - val_loss: 1.0083\n",
      "Epoch 9/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 487ms/step - accuracy: 0.5043 - loss: 1.0256 - val_accuracy: 0.5333 - val_loss: 1.0076\n",
      "Epoch 10/10\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 471ms/step - accuracy: 0.5191 - loss: 1.0109 - val_accuracy: 0.5333 - val_loss: 1.0118\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - accuracy: 0.5126 - loss: 1.0267\n",
      "Test Loss: 1.023231029510498\n",
      "Test Accuracy: 0.5188679099082947\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 157ms/step\n",
      "F1 Score: 0.3545\n",
      "Precision: 0.2692\n",
      "Recall: 0.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential([\n",
    "    LSTM(500, dropout = 0.2, recurrent_dropout=0.2),  # LSTM layer with 64 units,\n",
    "    # Dense(64, activation = 'relu'),\n",
    "    # Dropout(0.3),\n",
    "    Dense(3, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train, y_train, epochs=10, batch_size=20, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "#-----------------Test-----------------------\n",
    "evaluate_model(model_lstm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_tokenizer_keras, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - accuracy: 0.5138 - loss: 1.0017 - val_accuracy: 0.5792 - val_loss: 0.9385\n",
      "Epoch 2/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - accuracy: 0.5625 - loss: 0.9268 - val_accuracy: 0.6208 - val_loss: 0.8804\n",
      "Epoch 3/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.5864 - loss: 0.8929 - val_accuracy: 0.5708 - val_loss: 0.9020\n",
      "Epoch 4/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.5978 - loss: 0.8853 - val_accuracy: 0.6000 - val_loss: 0.8740\n",
      "Epoch 5/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6164 - loss: 0.8414 - val_accuracy: 0.6292 - val_loss: 0.8664\n",
      "Epoch 6/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.6148 - loss: 0.8490 - val_accuracy: 0.5708 - val_loss: 0.9326\n",
      "Epoch 7/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.6554 - loss: 0.7863 - val_accuracy: 0.6083 - val_loss: 0.8627\n",
      "Epoch 8/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - accuracy: 0.6499 - loss: 0.7927 - val_accuracy: 0.5875 - val_loss: 0.9153\n",
      "Epoch 9/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6555 - loss: 0.7773 - val_accuracy: 0.6125 - val_loss: 0.8792\n",
      "Epoch 10/10\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6694 - loss: 0.7526 - val_accuracy: 0.6167 - val_loss: 0.8906\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.5658 - loss: 0.9284\n",
      "Test Loss: 0.8986704349517822\n",
      "Test Accuracy: 0.5825471878051758\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step\n",
      "F1 Score: 0.5638\n",
      "Precision: 0.5627\n",
      "Recall: 0.5825\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = glovee_embedding_matrix.shape\n",
    "\n",
    "model0 = Sequential()\n",
    "# model0.add(Embedding(vocab_size, embedding_dim, embeddings_initializer=Constant(glovee_embedding_matrix), trainable=False))\n",
    "model0.add(Bidirectional(LSTM(80, dropout=0.5)))\n",
    "model0.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model0.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model0.fit(X_train, y_train, epochs=10, batch_size=20, validation_split=0.05)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "#-----------------Test-----------------------\n",
    "evaluate_model(model0, X_test, y_test)\n",
    "\n",
    "# model0.save('model0.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0obmama = model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0romney = model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 174ms/step - accuracy: 0.4746 - loss: 1.2183 - val_accuracy: 0.6083 - val_loss: 0.9005\n",
      "Epoch 2/8\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 171ms/step - accuracy: 0.5732 - loss: 0.9177 - val_accuracy: 0.5979 - val_loss: 1.0287\n",
      "Epoch 3/8\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 175ms/step - accuracy: 0.5910 - loss: 0.8945 - val_accuracy: 0.6146 - val_loss: 0.8918\n",
      "Epoch 4/8\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 178ms/step - accuracy: 0.6236 - loss: 0.8321 - val_accuracy: 0.6000 - val_loss: 0.8820\n",
      "Epoch 5/8\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.6342 - loss: 0.8045 - val_accuracy: 0.6083 - val_loss: 0.9333\n",
      "Epoch 6/8\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.6601 - loss: 0.7744 - val_accuracy: 0.6229 - val_loss: 0.8898\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5831 - loss: 0.9411\n",
      "Test Loss: 0.9161748290061951\n",
      "Test Accuracy: 0.5966981053352356\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "max_length = TWEET_MAX_LENGTH\n",
    "embedding_dim = WORD_EMBEDDING_SIZE  # Assuming you have 100-dimensional pre-embedded tweets\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 1\n",
    "output_dim = 3  # Number of output classes\n",
    "\n",
    "# Input layer\n",
    "# inputs = Input(shape=(max_length,))\n",
    "inputs = Input(shape=(TWEET_MAX_LENGTH, WORD_EMBEDDING_SIZE))\n",
    "\n",
    "# embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "\n",
    "# Transformer encoder\n",
    "# encoder_output = embedding_layer\n",
    "encoder_output = inputs\n",
    "for _ in range(num_layers):\n",
    "    encoder_output = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embedding_dim)(encoder_output, encoder_output)\n",
    "    encoder_output = tf.keras.layers.Dropout(0.3)(encoder_output)\n",
    "    encoder_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_output)\n",
    "    ff_output = tf.keras.layers.Dense(ff_dim, activation='relu')(encoder_output)\n",
    "    encoder_output = tf.keras.layers.Dropout(0.3)(ff_output)\n",
    "    encoder_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_output)\n",
    "\n",
    "# Global average pooling and output layer\n",
    "pooled_output = GlobalAveragePooling1D()(encoder_output)\n",
    "outputs = Dense(output_dim, activation='softmax')(pooled_output)\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=8, batch_size=32, validation_split=0.1, callbacks=[EarlyStopping(patience=2)])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.8144 - loss: 0.4896\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8465 - loss: 0.4169\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.8896 - loss: 0.3213\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - accuracy: 0.9083 - loss: 0.2638\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9308 - loss: 0.2232\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.8693 - loss: 0.3842\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.9203 - loss: 0.2425\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9394 - loss: 0.1836\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9471 - loss: 0.1565\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.9667 - loss: 0.1144\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 479ms/step - accuracy: 0.5101 - loss: 1.0230\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 475ms/step - accuracy: 0.5081 - loss: 1.0239\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 467ms/step - accuracy: 0.5009 - loss: 1.0216\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 461ms/step - accuracy: 0.5160 - loss: 1.0125\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 471ms/step - accuracy: 0.5141 - loss: 1.0202\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanuj\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - accuracy: 0.6692 - loss: 0.7651\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - accuracy: 0.6832 - loss: 0.7303\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 70ms/step - accuracy: 0.6996 - loss: 0.7112\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.7134 - loss: 0.6837\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.7083 - loss: 0.6806\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 174ms/step - accuracy: 0.6597 - loss: 0.7668\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 169ms/step - accuracy: 0.6691 - loss: 0.7589\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 166ms/step - accuracy: 0.6738 - loss: 0.7373\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.7084 - loss: 0.6980\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 172ms/step - accuracy: 0.6885 - loss: 0.7045\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n",
      "Model: Simple FF NN\n",
      "  Accuracy: 0.9083\n",
      "  Precision: 0.9087\n",
      "  Recall: 0.9083\n",
      "  F1 Score: 0.9078\n",
      "Model: Convolution\n",
      "  Accuracy: 0.9319\n",
      "  Precision: 0.9327\n",
      "  Recall: 0.9319\n",
      "  F1 Score: 0.9317\n",
      "Model: lstm\n",
      "  Accuracy: 0.5110\n",
      "  Precision: 0.2612\n",
      "  Recall: 0.5110\n",
      "  F1 Score: 0.3457\n",
      "Model: bi-lstm\n",
      "  Accuracy: 0.7444\n",
      "  Precision: 0.7458\n",
      "  Recall: 0.7444\n",
      "  F1 Score: 0.7365\n",
      "Model: encoder\n",
      "  Accuracy: 0.6696\n",
      "  Precision: 0.6698\n",
      "  Recall: 0.6696\n",
      "  F1 Score: 0.6505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compare_techniques(X_train, y_train, models):\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            \n",
    "            # Convert probability distributions to class labels\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            \n",
    "            accuracy = accuracy_score(y_val_fold, y_pred_classes)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_val_fold, y_pred_classes, average='weighted')\n",
    "            \n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        results[name] = {\n",
    "            'Accuracy': np.mean(accuracies),\n",
    "            'Precision': np.mean(precisions),\n",
    "            'Recall': np.mean(recalls),\n",
    "            'F1 Score': np.mean(f1_scores)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "models = {\n",
    "    'Simple FF NN': model_ff_nn,\n",
    "    'Convolution': model_conv_nn,\n",
    "    'lstm': model_lstm,\n",
    "    'bi-lstm': model0,\n",
    "    'encoder': model\n",
    "}\n",
    "\n",
    "results = compare_techniques(X_train, y_train, models)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {metrics['F1 Score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"id\", \"tweet\"]\n",
    "\n",
    "dbtest = pd.read_csv('final-testData-no-label-Romney-tweets.csv', header=None)\n",
    "\n",
    "# Rename the columns you care about\n",
    "dbtest.rename(columns={0: \"id\", 1: \"tweet\"}, inplace=True)\n",
    "\n",
    "# Drop the empty columns\n",
    "dbtest.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt; got 3 less minutes and had to de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;e&gt;Mitt  &lt;/e&gt;is beating him UP!  on his record...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I actually like  &lt;e&gt;Romney &lt;/e&gt;'s response to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Just for that &lt;a&gt;immigration statement &lt;/a&gt;tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>This man  &lt;e&gt;Romney  &lt;/e&gt;is tearing this dude ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>1896</td>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt;'s \"\"One Point Plan\"\" Is Mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>1897</td>\n",
       "      <td>\\t@GStephanopoulos I'd like &lt;a&gt;straight answer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>1898</td>\n",
       "      <td>Gangbanger: Ay Homie.... Who you roll with?!?....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>1899</td>\n",
       "      <td>The reason Ann &lt;e&gt;Romney&lt;/e&gt;is so confused is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>1900</td>\n",
       "      <td>@Michelle&lt;e&gt;obama&lt;/e&gt; &lt;e&gt;Romney&lt;/e&gt;should resp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "0        1  <e>Romney</e> got 3 less minutes and had to de...\n",
       "1        2  <e>Mitt  </e>is beating him UP!  on his record...\n",
       "2        3  I actually like  <e>Romney </e>'s response to ...\n",
       "3        4  Just for that <a>immigration statement </a>tha...\n",
       "4        5  This man  <e>Romney  </e>is tearing this dude ...\n",
       "...    ...                                                ...\n",
       "1895  1896  <e>Mitt Romney</e>'s \"\"One Point Plan\"\" Is Mak...\n",
       "1896  1897  \\t@GStephanopoulos I'd like <a>straight answer...\n",
       "1897  1898  Gangbanger: Ay Homie.... Who you roll with?!?....\n",
       "1898  1899  The reason Ann <e>Romney</e>is so confused is ...\n",
       "1899  1900  @Michelle<e>obama</e> <e>Romney</e>should resp...\n",
       "\n",
       "[1900 rows x 2 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbtest = preprocess_tweets(dbtest)\n",
    "\n",
    "\n",
    "tokenized_tweets_keras_test = get_tokenized_tweets(word_tokenizer_keras, dbtest['tweet'])\n",
    "\n",
    "tokenized_tweets_keras_test = pad_sequences(tokenized_tweets_keras_test, maxlen = TWEET_MAX_LENGTH, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>romney got less minutes debate candy crowley s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>mitt beating record credibility character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>actually like romney 's response immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>immigration statement romney answered 18 enoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>man romney tearing dude economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>1896</td>\n",
       "      <td>mitt romney 's `` '' one point plan '' '' maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>1897</td>\n",
       "      <td>'d like straight answers romneyon whether beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>1898</td>\n",
       "      <td>gangbanger ay homie .. roll .. romney never mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>1899</td>\n",
       "      <td>reason ann romneyis confused mitt romneyflip f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>1900</td>\n",
       "      <td>romneyshould respect religious belief respectn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "0        1  romney got less minutes debate candy crowley s...\n",
       "1        2          mitt beating record credibility character\n",
       "2        3       actually like romney 's response immigration\n",
       "3        4  immigration statement romney answered 18 enoug...\n",
       "4        5                  man romney tearing dude economics\n",
       "...    ...                                                ...\n",
       "1895  1896  mitt romney 's `` '' one point plan '' '' maki...\n",
       "1896  1897  'd like straight answers romneyon whether beli...\n",
       "1897  1898  gangbanger ay homie .. roll .. romney never mi...\n",
       "1898  1899  reason ann romneyis confused mitt romneyflip f...\n",
       "1899  1900  romneyshould respect religious belief respectn...\n",
       "\n",
       "[1900 rows x 2 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 64ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model0romney.predict(tokenized_tweets_keras_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 3)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 65ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get the predicted class labels\n",
    "y_pred_classes = np.argmax(model0romney.predict(tokenized_tweets_keras_test), axis=-1)\n",
    "\n",
    "# Create a mapping dictionary\n",
    "mapping_dict = {0: -1, 1: 0, 2: 1}\n",
    "\n",
    "# Replace the predicted class labels according to the mapping dictionary\n",
    "y_pred_classes_mapped = [mapping_dict[label] for label in y_pred_classes]\n",
    "\n",
    "# Create a DataFrame where the first column is the ids and the second column is the mapped predicted class labels\n",
    "id_class_df = pd.DataFrame({'id': dbtest['id'], 'class_label': y_pred_classes_mapped})\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "# id_class_df.to_csv('romney_test.csv', index=False)\n",
    "\n",
    "def write_output_file(output_filename, tweet_ids, predicted_sentiments):\n",
    "    with open(output_filename, 'w') as file:\n",
    "        for tweet_id, sentiment in zip(tweet_ids, predicted_sentiments):\n",
    "            file.write(f\"({tweet_id} {sentiment})\\n\")\n",
    "\n",
    "write_output_file('romney_test.txt', dbtest['id'], y_pred_classes_mapped)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
